#!/usr/bin/env python3
"""
Renderç‰ˆ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¯æœ4æ™‚å®Ÿè¡Œ - ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°â†’AIè¦ç´„â†’ãƒ‡ãƒ¼ã‚¿æ›´æ–°
"""

import sys
import os
import json
import traceback
from datetime import datetime
from pathlib import Path
import requests
from bs4 import BeautifulSoup
import time
import random

# ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
print("=== ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ===")
print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}")
print(f"Pythonå®Ÿè¡Œãƒ‘ã‚¹: {sys.executable}")
print(f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {Path(__file__).parent}")
print(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
print("=" * 50)

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / 'data'
DATA_DIR.mkdir(exist_ok=True)

class RenderNewsAutomation:
    """Renderç”¨ãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–ã‚¯ãƒ©ã‚¹ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.processed_file = DATA_DIR / 'processed_news.json'
        self.report_file = DATA_DIR / 'daily_report.json'
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š
        self.rate_limits = {
            'mhlw.go.jp': 2.0,
            'nenkin.go.jp': 2.0,
            'news.yahoo.co.jp': 3.0
        }
        self.request_times = {}
        
        # ã‚¨ãƒ©ãƒ¼è¿½è·¡
        self.error_count = {}
        
        # ã‚ˆã‚Šè©³ç´°ãªã‚«ãƒ†ã‚´ãƒªå®šç¾©
        self.detailed_categories = {
            "å¥åº·ä¿é™º": {
                "keywords": ["å¥åº·ä¿é™º", "å”ä¼šã‘ã‚“ã½", "çµ„åˆå¥ä¿", "åŒ»ç™‚ä¿é™º", "ä¿é™ºæ–™ç‡", "è¢«æ‰¶é¤Šè€…"],
                "subcategories": ["ä¿é™ºæ–™", "çµ¦ä»˜", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "åšç”Ÿå¹´é‡‘": {
                "keywords": ["åšç”Ÿå¹´é‡‘", "è€é½¢å¹´é‡‘", "éšœå®³å¹´é‡‘", "éºæ—å¹´é‡‘", "å¹´é‡‘ä¿é™ºæ–™", "å—çµ¦"],
                "subcategories": ["ä¿é™ºæ–™", "çµ¦ä»˜", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "é›‡ç”¨ä¿é™º": {
                "keywords": ["é›‡ç”¨ä¿é™º", "å¤±æ¥­çµ¦ä»˜", "è‚²å…ä¼‘æ¥­çµ¦ä»˜", "ä»‹è­·ä¼‘æ¥­çµ¦ä»˜", "æ•™è‚²è¨“ç·´çµ¦ä»˜"],
                "subcategories": ["çµ¦ä»˜", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´", "ä¿é™ºæ–™"]
            },
            "åŠ´ç½ä¿é™º": {
                "keywords": ["åŠ´ç½ä¿é™º", "åŠ´åƒç½å®³", "æ¥­å‹™ç½å®³", "é€šå‹¤ç½å®³", "åŠ´ç½çµ¦ä»˜"],
                "subcategories": ["çµ¦ä»˜", "èªå®š", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "ä»‹è­·ä¿é™º": {
                "keywords": ["ä»‹è­·ä¿é™º", "è¦ä»‹è­·", "è¦æ”¯æ´", "ä»‹è­·çµ¦ä»˜", "ä»‹è­·å ±é…¬"],
                "subcategories": ["çµ¦ä»˜", "èªå®š", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "ç¤¾ä¼šä¿é™ºå…¨èˆ¬": {
                "keywords": ["ç¤¾ä¼šä¿é™º", "é©ç”¨æ‹¡å¤§", "è¢«ä¿é™ºè€…", "äº‹æ¥­ä¸»", "åˆ¶åº¦æ”¹æ­£", "æ³•æ”¹æ­£"],
                "subcategories": ["åˆ¶åº¦å¤‰æ›´", "é©ç”¨", "æ‰‹ç¶šã", "æ³•ä»¤"]
            }
        }
        
        # é‡è¦åº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.importance_keywords = {
            "é«˜": ["æ³•æ”¹æ­£", "åˆ¶åº¦æ”¹æ­£", "æ–°è¨­", "å»ƒæ­¢", "æ–½è¡Œ", "å…¬å¸ƒ", "çœä»¤", "å‘Šç¤º", "æ”¹å®š"],
            "ä¸­": ["è¦‹ç›´ã—", "æ‹¡å¤§", "ç¸®å°", "æ–™ç‡å¤‰æ›´", "åŸºæº–å¤‰æ›´", "æ¡ˆ", "äºˆå®š"],
            "ä½": ["æ‰‹ç¶šã", "æ§˜å¼", "ãŠçŸ¥ã‚‰ã›", "æ¡ˆå†…", "èª¬æ˜ä¼š", "ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆ"]
        }
    
    def check_rate_limit(self, domain):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        now = datetime.now()
        
        if domain in self.request_times:
            last_request = self.request_times[domain]
            required_interval = self.rate_limits.get(domain, 2.0)
            
            elapsed = (now - last_request).total_seconds()
            if elapsed < required_interval:
                wait_time = required_interval - elapsed
                print(f"â³ {domain} ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {wait_time:.1f}ç§’å¾…æ©Ÿ")
                time.sleep(wait_time)
        
        self.request_times[domain] = now
    
    def scrape_mhlw_comprehensive(self):
        """åšç”ŸåŠ´åƒçœç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        news_list = []
        
        try:
            print("ğŸ›ï¸ åšç”ŸåŠ´åƒçœãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹ï¼ˆå¼·åŒ–ç‰ˆï¼‰")
            
            # ã‚ˆã‚Šå¤šãã®åšåŠ´çœãƒšãƒ¼ã‚¸ã‚’å¯¾è±¡ã«ã™ã‚‹
            mhlw_urls = [
                ('https://www.mhlw.go.jp/stf/houdou/houdou_list.html', 'ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹'),
                ('https://www.mhlw.go.jp/stf/new-info/index.html', 'æ–°ç€æƒ…å ±'),
                ('https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/kenkou_iryou/iryouhoken/index.html', 'åŒ»ç™‚ä¿é™º'),
                ('https://www.nenkin.go.jp/info/index.html', 'å¹´é‡‘æ©Ÿæ§‹æ–°ç€')
            ]
            
            for url, page_type in mhlw_urls:
                try:
                    domain = url.split('/')[2]
                    self.check_rate_limit(domain)
                    
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    print(f"  ğŸ“„ {page_type}ãƒšãƒ¼ã‚¸ã‚’è§£æä¸­...")
                    
                    # ã‚ˆã‚ŠåŠ¹æœçš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®æŠ½å‡º
                    links = soup.find_all('a', href=True)
                    extracted = 0
                    
                    for link in links[:50]:  # ã‚ˆã‚Šå¤šãæ¤œæŸ»
                        href = link.get('href', '')
                        title = link.get_text(strip=True)
                        
                        # ã‚ˆã‚Šå³æ ¼ãªç¤¾ä¼šä¿é™ºé–¢é€£ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if self.is_social_insurance_relevant(title):
                            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                            if href.startswith('/'):
                                if 'mhlw.go.jp' in url:
                                    full_url = 'https://www.mhlw.go.jp' + href
                                elif 'nenkin.go.jp' in url:
                                    full_url = 'https://www.nenkin.go.jp' + href
                                else:
                                    full_url = href
                            else:
                                full_url = href
                            
                            # ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±æŠ½å‡º
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'å¹´é‡‘æ©Ÿæ§‹' if 'nenkin.go.jp' in url else 'åšç”ŸåŠ´åƒçœ',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence,
                                'content_length': len(title),
                                'page_type': page_type
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                            if extracted >= 10:  # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€å¤§10ä»¶
                                break
                    
                    print(f"  âœ… {page_type}: {extracted}ä»¶å–å¾—")
                    
                except Exception as e:
                    print(f"  âŒ {page_type}å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                    self.error_count[url] = self.error_count.get(url, 0) + 1
                    continue
            
            print(f"âœ… åšåŠ´çœç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(news_list)}ä»¶å–å¾—")
            return news_list
            
        except Exception as e:
            print(f"âŒ åšåŠ´çœç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_yahoo_news(self):
        """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ ç¤¾ä¼šä¿é™ºé–¢é€£å–å¾—"""
        news_list = []
        
        try:
            print("ğŸ“º Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
            
            # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ï¼ˆç¤¾ä¼šä¿é™ºé–¢é€£ï¼‰
            search_terms = ['ç¤¾ä¼šä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'å¹´é‡‘æ”¹æ­£']
            
            for term in search_terms:
                try:
                    search_url = f"https://news.yahoo.co.jp/search?p={term}&ei=UTF-8"
                    response = self.session.get(search_url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ç¾åœ¨ã®Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ§‹é€ ã«å¯¾å¿œã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
                    articles = soup.find_all('article') or soup.find_all('div', class_=lambda x: x and ('sc-' in x or 'newsFeed' in x))
                    
                    for article in articles[:5]:  # å„æ¤œç´¢èªã§5ä»¶
                        try:
                            # ã‚ˆã‚Šå¹…åºƒã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆç¾åœ¨ã®Yahooæ§‹é€ ã«å¯¾å¿œï¼‰
                            link_elem = (
                                article.find('a', href=True) or 
                                article.find('a') or
                                article.find('h1')
                            )
                            
                            if not link_elem:
                                continue
                            
                            title = link_elem.get_text(strip=True)
                            url = link_elem.get('href', '') if link_elem.name == 'a' else ''
                            
                            # URLãŒç©ºã®å ´åˆã¯è¦ªè¦ç´ ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                            if not url:
                                parent_link = article.find_parent('a') or article.find('a')
                                url = parent_link.get('href', '') if parent_link else ''
                            
                            if url.startswith('/'):
                                url = 'https://news.yahoo.co.jp' + url
                            elif not url.startswith('http'):
                                continue  # ç„¡åŠ¹ãªURLã¯ã‚¹ã‚­ãƒƒãƒ—
                            
                            news_item = {
                                'title': title,
                                'url': url,
                                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'category': self.categorize_news(title),
                                'importance': self.assess_importance(title),
                                'summary': f"ã€{self.categorize_news(title)}ã€‘ {title[:60]}...",
                                'keywords': self.extract_keywords(title),
                                'published_date': 'None',
                                'scraped_at': datetime.now().isoformat()
                            }
                            
                            news_list.append(news_item)
                            
                        except Exception as e:
                            print(f"Yahooè¨˜äº‹è§£æã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    time.sleep(3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                except Exception as e:
                    print(f"Yahooæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {term}: {e}")
                    continue
            
            # é‡è¤‡é™¤å»
            unique_news = []
            seen_titles = set()
            for news in news_list:
                if news['title'] not in seen_titles and len(news['title']) > 10:
                    unique_news.append(news)
                    seen_titles.add(news['title'])
            
            print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(unique_news)}ä»¶å–å¾—ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
            if unique_news:
                print(f"ğŸ“ å–å¾—ä¾‹: {unique_news[0]['title'][:50]}...")
            return unique_news
            
        except Exception as e:
            print(f"âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def is_social_insurance_relevant(self, title):
        """ç¤¾ä¼šä¿é™ºé–¢é€£ã®åˆ¤å®šï¼ˆå¤§å¹…ç·©å’Œç‰ˆï¼‰"""
        if len(title.strip()) < 5:  # ã‚ˆã‚ŠçŸ­ã„é–¾å€¤
            return False
        
        # ç¤¾ä¼šä¿é™ºé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¤§å¹…æ‹¡å¼µç‰ˆï¼‰
        relevant_keywords = [
            # åŸºæœ¬çš„ãªç¤¾ä¼šä¿é™ºåˆ¶åº¦
            'å¥åº·ä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å›½æ°‘å¹´é‡‘', 'é›‡ç”¨ä¿é™º', 'åŠ´ç½ä¿é™º', 'ä»‹è­·ä¿é™º',
            'ç¤¾ä¼šä¿é™º', 'è¢«ä¿é™ºè€…', 'ä¿é™ºæ–™', 'å¹´é‡‘', 'åŒ»ç™‚ä¿é™º', 'å¤±æ¥­çµ¦ä»˜',
            'åŠ´åƒç½å®³', 'ä»‹è­·çµ¦ä»˜', 'ä¿é™ºé©ç”¨', 'åˆ¶åº¦æ”¹æ­£', 'æ³•æ”¹æ­£',
            
            # çµ„ç¹”ãƒ»åˆ¶åº¦å
            'å”ä¼šã‘ã‚“ã½', 'å¹´é‡‘æ©Ÿæ§‹', 'ç¤¾åŠ´å£«', 'çµ¦ä»˜é‡‘', 'é©ç”¨æ‹¡å¤§',
            'åšç”ŸåŠ´åƒçœ', 'ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯', 'å¹´é‡‘äº‹å‹™æ‰€', 'ç¤¾ä¼šä¿é™ºåŠ´å‹™å£«',
            
            # é–¢é€£ã™ã‚‹ç¤¾ä¼šå•é¡Œãƒ»æ”¿ç­–ï¼ˆå¤§å¹…æ‹¡å¼µï¼‰
            'åŒ»ç™‚è²»', 'é«˜é½¢è€…', 'éšœå®³è€…', 'è‚²å…', 'å‡ºç”£', 'ç™‚é¤Šè²»',
            'æ‰¶é¤Š', 'è³ƒé‡‘', 'åŠ´åƒè€…', 'äº‹æ¥­ä¸»', 'ä¿é™ºåˆ¶åº¦', 'ç¤¾ä¼šä¿éšœ',
            'ç¦ç¥‰', 'åŒ»ç™‚åˆ¶åº¦', 'é€€è·', 'å°±è·', 'è·æ¥­è¨“ç·´',
            'ä¼‘æ¥­è£œå„Ÿ', 'é€šå‹¤ç½å®³', 'æ¥­å‹™ç½å®³', 'è¦ä»‹è­·', 'è¦æ”¯æ´',
            
            # ã‚ˆã‚Šå¹…åºƒã„é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            'åƒãæ–¹æ”¹é©', 'å°‘å­é«˜é½¢åŒ–', 'ãƒã‚¤ãƒŠãƒ³ãƒãƒ¼', 'é›»å­ç”³è«‹',
            'å®šå¹´å»¶é•·', 'éæ­£è¦é›‡ç”¨', 'æ­£ç¤¾å“¡åŒ–', 'DX', 'ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–',
            'æ‰‹ç¶šãç°¡ç´ åŒ–', 'çª“å£', 'ç›¸è«‡', 'ç”³è«‹', 'çµ¦ä»˜',
            'èªå®š', 'å¯©æŸ»', 'æ”¯çµ¦', 'å—çµ¦', 'ç´ä»˜', 'å¾´å',
            
            # æ–°è¦è¿½åŠ ï¼ˆåŠ´åƒãƒ»çµŒæ¸ˆé–¢é€£ï¼‰
            'æœ€ä½è³ƒé‡‘', 'æ®‹æ¥­ä»£', 'æœ‰çµ¦ä¼‘æš‡', 'ç”£ä¼‘', 'è‚²ä¼‘', 'ä»‹è­·ä¼‘æš‡',
            'åŠ´åŸºæ³•', 'åŠ´åƒæ³•', 'åŠ´åƒåŸºæº–', 'æ™‚çŸ­å‹¤å‹™', 'ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯',
            'å‰¯æ¥­', 'å…¼æ¥­', 'è»¢è·', 'å°±æ´»', 'ãƒªã‚¹ãƒˆãƒ©', 'ãƒªã‚¹ã‚­ãƒªãƒ³ã‚°',
            'äººæ‰‹ä¸è¶³', 'æ¡ç”¨', 'æ–°å’', 'ä¸­é€”', 'ã‚·ãƒ‹ã‚¢', 'å¥³æ€§æ´»èº',
            'å¤šæ§˜æ€§', 'ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£', 'ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³', 'ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹',
            
            # çµŒæ¸ˆãƒ»æ”¿ç­–é–¢é€£
            'ç¨åˆ¶æ”¹æ­£', 'æ‰€å¾—ç¨', 'ä½æ°‘ç¨', 'æ¶ˆè²»ç¨', 'æ§é™¤', 'æ¸›ç¨',
            'ç‰©ä¾¡', 'ã‚¤ãƒ³ãƒ•ãƒ¬', 'æ™¯æ°—', 'çµŒæ¸ˆæ”¿ç­–', 'æ”¿åºœ', 'å›½ä¼š',
            'äºˆç®—', 'è²¡æ”¿', 'è£œåŠ©é‡‘', 'åŠ©æˆé‡‘', 'æ”¯æ´ç­–', 'ã‚³ãƒ­ãƒŠæ”¯æ´'
        ]
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé–¢ä¿‚ãªã„ã‚‚ã®ï¼‰
        exclude_keywords = [
            'JavaScript', 'Cookie', 'PDF', 'ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹',
            'ãƒ–ãƒ©ã‚¦ã‚¶', 'Internet Explorer', 'ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£',
            'ã‚¹ãƒãƒ¼ãƒ„', 'èŠ¸èƒ½', 'ã‚¨ãƒ³ã‚¿ãƒ¡', 'å¤©æ°—', 'å ã„', 'ã‚²ãƒ¼ãƒ ',
            'æ ªä¾¡', 'ç‚ºæ›¿', 'ç«¶é¦¬', 'å®ãã˜', 'ãƒ‘ãƒãƒ³ã‚³', 'ã‚¢ãƒ‹ãƒ¡', 'æ˜ ç”»'
        ]
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        for exclude in exclude_keywords:
            if exclude in title:
                return False
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆ1ã¤ã§ã‚‚å«ã¾ã‚Œã¦ã„ã‚Œã°OKï¼‰
        for keyword in relevant_keywords:
            if keyword in title:
                return True
        
        # éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆç·©ã„æ¡ä»¶ï¼‰
        partial_keywords = [
            'ä¿é™º', 'å¹´é‡‘', 'çµ¦ä»˜', 'åˆ¶åº¦', 'åŠ´åƒ', 'åŒ»ç™‚', 'ç¦ç¥‰',
            'åƒ', 'ä»•äº‹', 'é›‡ç”¨', 'é€€è·', 'å°±è·', 'ç—…æ°—', 'ä»‹è­·',
            'å­è‚²ã¦', 'å‡ºç”£', 'è‚²å…', 'éšœå®³', 'é«˜é½¢', 'ç”³è«‹', 'æ‰‹ç¶šã',
            'ç¨', 'æ–™é‡‘', 'æ”¯æ‰•ã„', 'æ§é™¤', 'æ¸›å…', 'å…é™¤', 'è£œåŠ©',
            'æ”¯æ´', 'åŠ©æˆ', 'æ”¿ç­–', 'æ”¹æ­£', 'å¤‰æ›´', 'è¦‹ç›´ã—'
        ]
        
        matched_partials = 0
        for keyword in partial_keywords:
            if keyword in title:
                matched_partials += 1
        
        # 1å€‹ä»¥ä¸Šã®éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°é–¢é€£ã¨ã™ã‚‹ï¼ˆå¤§å¹…ç·©å’Œï¼‰
        if matched_partials >= 1:
            return True
        
        return False
    
    def enhanced_categorization(self, title, content):
        """å¼·åŒ–ç‰ˆã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        text = title + " " + content
        
        category_scores = {}
        related_categories = []
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        for category, data in self.detailed_categories.items():
            score = 0
            matched_keywords = []
            
            for keyword in data["keywords"]:
                count = text.count(keyword)
                if count > 0:
                    score += count * (len(keyword) / 3)  # é•·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã»ã©é‡è¦
                    matched_keywords.append(keyword)
            
            if score > 0:
                category_scores[category] = {
                    "score": score,
                    "keywords": matched_keywords
                }
        
        if not category_scores:
            return "ãã®ä»–", [], 0.0
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’ä¸»ã‚«ãƒ†ã‚´ãƒªã«
        primary_category = max(category_scores.keys(), key=lambda k: category_scores[k]["score"])
        confidence = min(category_scores[primary_category]["score"] / 10, 1.0)
        
        # é–¢é€£ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¹ã‚³ã‚¢1.0ä»¥ä¸Šï¼‰
        for cat, data in category_scores.items():
            if cat != primary_category and data["score"] >= 1.0:
                related_categories.append(cat)
        
        return primary_category, related_categories, confidence
    
    def enhanced_importance(self, title, content):
        """å¼·åŒ–ç‰ˆé‡è¦åº¦åˆ¤å®š"""
        text = title + " " + content
        
        # é«˜é‡è¦åº¦ãƒã‚§ãƒƒã‚¯
        for keyword in self.importance_keywords["é«˜"]:
            if keyword in text:
                return "é«˜"
        
        # ä¸­é‡è¦åº¦ãƒã‚§ãƒƒã‚¯
        for keyword in self.importance_keywords["ä¸­"]:
            if keyword in text:
                return "ä¸­"
        
        return "ä½"
    
    def enhanced_keywords(self, title, content):
        """å¼·åŒ–ç‰ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        import re
        text = title + " " + content
        keywords = []
        
        # åˆ¶åº¦åãƒ»çµ„ç¹”åæŠ½å‡º
        institution_patterns = [
            r'å”ä¼šã‘ã‚“ã½', r'å¥åº·ä¿é™ºçµ„åˆ', r'å¹´é‡‘äº‹å‹™æ‰€', r'å¹´é‡‘æ©Ÿæ§‹',
            r'åšç”ŸåŠ´åƒçœ', r'ç¤¾ä¼šä¿é™ºåŠ´å‹™å£«', r'åŠ´åƒåŸºæº–ç›£ç£ç½²'
        ]
        
        for pattern in institution_patterns:
            if re.search(pattern, text):
                keywords.append(pattern)
        
        # é‡‘é¡ãƒ»ç‡ãƒ»æœŸé–“æŠ½å‡º
        numeric_patterns = [
            (r'\d+\.?\d*%', 'ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ'),
            (r'\d+(?:,\d+)*å††', 'é‡‘é¡'),
            (r'\d+(?:,\d+)*ä¸‡å††', 'ä¸‡å††å˜ä½'),
            (r'\d+å¹´\d+æœˆ', 'å¹´æœˆ'),
            (r'ä»¤å’Œ\d+å¹´', 'ä»¤å’Œå¹´'),
            (r'å¹³æˆ\d+å¹´', 'å¹³æˆå¹´')
        ]
        
        for pattern, desc in numeric_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches[:3])  # æœ€å¤§3å€‹ã¾ã§
        
        return list(set(keywords))[:10]  # é‡è¤‡é™¤å»ãƒ»ä¸Šä½10å€‹
    
    def create_summary(self, title, category, keywords):
        """è©³ç´°è¦ç´„æ–‡ä½œæˆ"""
        import re
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦éƒ¨åˆ†æŠ½å‡º
        title_clean = re.sub(r'ã«ã¤ã„ã¦$|ã«é–¢ã—ã¦$|ã®ãŠçŸ¥ã‚‰ã›$', '', title)
        
        summary_parts = []
        
        # ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤º
        summary_parts.append(f"ã€{category}ã€‘")
        
        # ä¸»è¦å†…å®¹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        if len(title_clean) > 20:
            title_summary = title_clean[:30] + "..."
        else:
            title_summary = title_clean
        summary_parts.append(title_summary)
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ 
        if keywords:
            key_info = []
            for keyword in keywords[:3]:
                if '%' in keyword or 'å††' in keyword or 'å¹´' in keyword:
                    key_info.append(keyword)
            
            if key_info:
                summary_parts.append(f"({', '.join(key_info)})")
        
        summary = " ".join(summary_parts)
        return summary[:120]  # 120æ–‡å­—åˆ¶é™
    
    def generate_id(self, title, url):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹IDç”Ÿæˆ"""
        import hashlib
        return hashlib.md5((title + url).encode()).hexdigest()[:8]
    
    def scrape_news_sites_comprehensive(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç·åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        news_list = []
        
        try:
            print("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç·åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            
            # è¤‡æ•°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰åé›†ï¼ˆYahooå¾©æ´» + ä»£æ›¿ã‚½ãƒ¼ã‚¹è¿½åŠ ï¼‰
            news_sites = [
                {
                    'name': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                    'base_url': 'https://news.yahoo.co.jp',
                    'search_terms': ['ç¤¾ä¼šä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'å¹´é‡‘æ”¹æ­£', 'ä»‹è­·ä¿é™º', 'åŠ´ç½ä¿é™º', 'å¹´é‡‘åˆ¶åº¦', 'åŒ»ç™‚ä¿é™ºåˆ¶åº¦', 'ä¿é™ºæ–™æ”¹æ­£', 'åƒãæ–¹æ”¹é©', 'æœ€ä½è³ƒé‡‘'],
                    'scraper': self.scrape_yahoo_simple
                },
                {
                    'name': 'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹',
                    'base_url': 'https://www3.nhk.or.jp',
                    'search_terms': ['ç¤¾ä¼šä¿é™º', 'å¹´é‡‘', 'åŒ»ç™‚ä¿é™º', 'é›‡ç”¨', 'åŠ´åƒ', 'åšç”ŸåŠ´åƒçœ'],
                    'scraper': self.scrape_nhk_news
                },
                {
                    'name': 'æ™‚äº‹é€šä¿¡',
                    'base_url': 'https://www.jiji.com',
                    'search_terms': ['ç¤¾ä¼šä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'å¹´é‡‘æ”¹æ­£', 'ä»‹è­·ä¿é™º', 'åŠ´ç½ä¿é™º'],
                    'scraper': self.scrape_jiji_news
                },
                {
                    'name': 'å…±åŒé€šä¿¡',
                    'base_url': 'https://www.kyodo.co.jp',
                    'search_terms': ['ç¤¾ä¼šä¿é™º', 'å¹´é‡‘åˆ¶åº¦', 'åŒ»ç™‚åˆ¶åº¦', 'é›‡ç”¨åˆ¶åº¦', 'åŠ´åƒæ”¿ç­–'],
                    'scraper': self.scrape_kyodo_news
                },
                {
                    'name': 'ITmedia ãƒ“ã‚¸ãƒã‚¹',
                    'base_url': 'https://www.itmedia.co.jp/business',
                    'search_terms': ['ç¤¾ä¼šä¿é™º', 'åƒãæ–¹æ”¹é©', 'åŠ´åƒæ³•', 'é›‡ç”¨ä¿é™º', 'åšç”Ÿå¹´é‡‘'],
                    'scraper': self.scrape_itmedia_news
                }
            ]
            
            for site_config in news_sites:
                try:
                    print(f"  ğŸ“± {site_config['name']}ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
                    site_news = site_config['scraper'](site_config)
                    
                    if site_news:
                        news_list.extend(site_news)
                        print(f"  âœ… {site_config['name']}: {len(site_news)}ä»¶å–å¾—")
                    else:
                        print(f"  âš ï¸ {site_config['name']}: ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—")
                    
                    time.sleep(2)  # ã‚µã‚¤ãƒˆé–“ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                except Exception as e:
                    print(f"  âŒ {site_config['name']}ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            unique_news = []
            seen_urls = set()
            for news in news_list:
                url = news.get('url', '')
                title = news.get('title', '')
                
                if (url and url not in seen_urls and 
                    self.is_social_insurance_relevant(title)):
                    seen_urls.add(url)
                    unique_news.append(news)
            
            print(f"âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç·åˆ {len(unique_news)}ä»¶å–å¾—ï¼ˆé‡è¤‡é™¤å»ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰")
            return unique_news
            
        except Exception as e:
            print(f"âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç·åˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_yahoo_enhanced(self, site_config):
        """å¼·åŒ–ç‰ˆYahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('news.yahoo.co.jp')
            
            for term in site_config['search_terms']:
                try:
                    search_url = f"{site_config['base_url']}/search?p={term}&ei=UTF-8"
                    response = self.session.get(search_url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # å¼·åŒ–ã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
                    article_selectors = [
                        'article',
                        'div[class*="sc-"]',
                        'div[class*="newsFeed"]',
                        'li[class*="topics"]',
                        'div.contentMain'
                    ]
                    
                    articles = []
                    for selector in article_selectors:
                        found = soup.select(selector)
                        if found:
                            articles = found
                            break
                    
                    extracted = 0
                    for article in articles[:10]:  # å„æ¤œç´¢èªã§10ä»¶
                        try:
                            # ãƒªãƒ³ã‚¯ã¨ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
                            link_selectors = ['a[href*="news"]', 'a[href]', 'h1 a', 'h2 a', 'h3 a']
                            link_elem = None
                            
                            for selector in link_selectors:
                                link_elem = article.select_one(selector)
                                if link_elem:
                                    break
                            
                            if not link_elem:
                                continue
                            
                            title = link_elem.get_text(strip=True)
                            url = link_elem.get('href', '')
                            
                            if not title or len(title) < 10:
                                continue
                            
                            # URLæ­£è¦åŒ–
                            if url.startswith('/'):
                                url = site_config['base_url'] + url
                            elif not url.startswith('http'):
                                continue
                            
                            # æ—¥ä»˜æŠ½å‡ºè©¦ã¿
                            date_elem = article.select_one('time, .date, [class*="date"], [class*="time"]')
                            published_date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
                            
                            # è©³ç´°æƒ…å ±ç”Ÿæˆ
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, url),
                                'title': title,
                                'url': url,
                                'source': site_config['name'],
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': published_date,
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence,
                                'search_term': term
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            print(f"    Yahooè¨˜äº‹è§£æã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    print(f"    æ¤œç´¢èª '{term}': {extracted}ä»¶")
                    time.sleep(1)  # æ¤œç´¢é–“ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                except Exception as e:
                    print(f"    Yahooæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {term}: {e}")
                    continue
            
            return news_list
            
        except Exception as e:
            print(f"Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_nhk_news(self, site_config):
        """NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('www3.nhk.or.jp')
            
            # NHKã®ç¤¾ä¼šãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¿®æ­£ç‰ˆï¼‰
            nhk_urls = [
                'https://www3.nhk.or.jp/news/',  # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
                'https://www3.nhk.or.jp/news/catnew.html'  # æ–°ç€ãƒ‹ãƒ¥ãƒ¼ã‚¹
            ]
            
            for url in nhk_urls:
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # NHKã®ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®æŠ½å‡º
                    articles = soup.select('.content-item, .module-list li, article')
                    
                    extracted = 0
                    for article in articles[:10]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            # URLæ­£è¦åŒ–
                            if href.startswith('/'):
                                full_url = 'https://www3.nhk.or.jp' + href
                            else:
                                full_url = href
                            
                            # æ—¥ä»˜æŠ½å‡º
                            date_elem = article.select_one('.date, time, [class*="time"]')
                            published_date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': published_date,
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            print(f"    NHKè¨˜äº‹è§£æã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    print(f"    NHK {url.split('/')[-1]}: {extracted}ä»¶")
                    
                except Exception as e:
                    print(f"    NHK URLã‚¨ãƒ©ãƒ¼ {url}: {e}")
                    continue
            
            return news_list
            
        except Exception as e:
            print(f"NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_sankei_news(self, site_config):
        """ç”£çµŒãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('www.sankei.com')
            
            # ç”£çµŒãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ¤œç´¢
            for term in site_config['search_terms']:
                try:
                    search_url = f"https://www.sankei.com/search/{term}/"
                    response = self.session.get(search_url, timeout=30)
                    
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ç”£çµŒã®è¨˜äº‹ä¸€è¦§
                    articles = soup.select('.story-list li, .article-list li, article')
                    
                    extracted = 0
                    for article in articles[:5]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            if href.startswith('/'):
                                full_url = 'https://www.sankei.com' + href
                            else:
                                full_url = href
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'ç”£çµŒãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence,
                                'search_term': term
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            continue
                    
                    print(f"    ç”£çµŒ '{term}': {extracted}ä»¶")
                    time.sleep(1)
                    
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            return []
    
    def scrape_asahi_news(self, site_config):
        """æœæ—¥æ–°èãƒ‡ã‚¸ã‚¿ãƒ« ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('www.asahi.com')
            
            # æœæ—¥æ–°èã®æ¤œç´¢
            for term in site_config['search_terms']:
                try:
                    search_url = f"https://www.asahi.com/search/news/?query={term}"
                    response = self.session.get(search_url, timeout=30)
                    
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æœæ—¥æ–°èã®è¨˜äº‹ä¸€è¦§
                    articles = soup.select('.SearchResult li, article, .List li')
                    
                    extracted = 0
                    for article in articles[:5]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            if href.startswith('/'):
                                full_url = 'https://www.asahi.com' + href
                            else:
                                full_url = href
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'æœæ—¥æ–°èãƒ‡ã‚¸ã‚¿ãƒ«',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence,
                                'search_term': term
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            continue
                    
                    print(f"    æœæ—¥ '{term}': {extracted}ä»¶")
                    time.sleep(1)
                    
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            return []
    
    def scrape_yahoo_simple(self, site_config):
        """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‚·ãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆRSS/ãƒˆãƒ”ãƒƒã‚¯åˆ©ç”¨ï¼‰"""
        news_list = []
        
        try:
            self.check_rate_limit('news.yahoo.co.jp')
            
            # Yahoo!ã®ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã¨RSSçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            yahoo_topics = [
                'https://news.yahoo.co.jp/topics/domestic',     # å›½å†…ãƒ‹ãƒ¥ãƒ¼ã‚¹
                'https://news.yahoo.co.jp/topics/business',     # çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹
                'https://news.yahoo.co.jp/topics/local'         # åœ°åŸŸãƒ‹ãƒ¥ãƒ¼ã‚¹
            ]
            
            for topic_url in yahoo_topics:
                try:
                    response = self.session.get(topic_url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ã‚ˆã‚Šåºƒç¯„å›²ã«ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    links = soup.find_all('a', href=True)
                    
                    extracted = 0
                    for link in links:
                        try:
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            # æœ€ä½é™ã®æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                            if (not title or len(title) < 10 or 
                                not href or 'news' not in href):
                                continue
                            
                            # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç·©å’Œç‰ˆã‚’ä½¿ç”¨ï¼‰
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            # URLæ­£è¦åŒ–
                            if href.startswith('/'):
                                full_url = 'https://news.yahoo.co.jp' + href
                            elif not href.startswith('http'):
                                continue
                            else:
                                full_url = href
                            
                            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ä½œæˆ
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                            # å„ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰æœ€å¤§5ä»¶
                            if extracted >= 5:
                                break
                                
                        except Exception as e:
                            continue
                    
                    print(f"    Yahoo {topic_url.split('/')[-1]}: {extracted}ä»¶")
                    time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                except Exception as e:
                    continue
            
            # é‡è¤‡é™¤å»
            unique_news = []
            seen_titles = set()
            for news in news_list:
                if news['title'] not in seen_titles:
                    unique_news.append(news)
                    seen_titles.add(news['title'])
            
            return unique_news
            
        except Exception as e:
            print(f"Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç°¡æ˜“ç‰ˆã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def scrape_nikkei_news(self, site_config):
        """æ—¥çµŒæ–°è ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆç„¡åŠ¹åŒ–ï¼‰"""
        # æ—¥çµŒæ–°èã¯æœ‰æ–™ä¼šå“¡åˆ¶ã®ãŸã‚ç„¡åŠ¹åŒ–
        return []
    
    def scrape_jiji_news(self, site_config):
        """æ™‚äº‹é€šä¿¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('www.jiji.com')
            
            # æ™‚äº‹é€šä¿¡ã®ç¤¾ä¼šã‚»ã‚¯ã‚·ãƒ§ãƒ³
            jiji_urls = [
                'https://www.jiji.com/jc/list?g=soc',  # ç¤¾ä¼š
                'https://www.jiji.com/jc/list?g=pol'   # æ”¿æ²»ï¼ˆåˆ¶åº¦é–¢é€£ï¼‰
            ]
            
            for url in jiji_urls:
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æ™‚äº‹é€šä¿¡ã®è¨˜äº‹ä¸€è¦§
                    articles = soup.select('.ListNewsItem, .newslist li, article')
                    
                    extracted = 0
                    for article in articles[:15]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            if href.startswith('/'):
                                full_url = 'https://www.jiji.com' + href
                            else:
                                full_url = href
                            
                            # æ—¥ä»˜æŠ½å‡º
                            date_elem = article.select_one('.date, time, [class*="time"]')
                            published_date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'æ™‚äº‹é€šä¿¡',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': published_date,
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            continue
                    
                    print(f"    æ™‚äº‹é€šä¿¡ {url.split('=')[-1]}: {extracted}ä»¶")
                    
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            return []
    
    def scrape_kyodo_news(self, site_config):
        """å…±åŒé€šä¿¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('www.kyodo.co.jp')
            
            # å…±åŒé€šä¿¡ã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰
            kyodo_urls = [
                'https://www.kyodo.co.jp/',
                'https://www.kyodo.co.jp/politics/',
                'https://www.kyodo.co.jp/life/'
            ]
            
            for url in kyodo_urls:
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # å…±åŒé€šä¿¡ã®è¨˜äº‹ä¸€è¦§
                    articles = soup.select('.article-list li, .news-list li, article, .card')
                    
                    extracted = 0
                    for article in articles[:10]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            if href.startswith('/'):
                                full_url = 'https://www.kyodo.co.jp' + href
                            elif not href.startswith('http'):
                                continue
                            else:
                                full_url = href
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'å…±åŒé€šä¿¡',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            continue
                    
                    print(f"    å…±åŒé€šä¿¡ {url.split('/')[-2] if len(url.split('/')) > 3 else 'main'}: {extracted}ä»¶")
                    
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            return []
    
    def scrape_itmedia_news(self, site_config):
        """ITmedia ãƒ“ã‚¸ãƒã‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('www.itmedia.co.jp')
            
            # ITmediaãƒ“ã‚¸ãƒã‚¹ã®ãƒˆãƒ”ãƒƒã‚¯
            itmedia_urls = [
                'https://www.itmedia.co.jp/business/',
                'https://www.itmedia.co.jp/business/subtop/work/'
            ]
            
            for url in itmedia_urls:
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ITmediaã®è¨˜äº‹ä¸€è¦§
                    articles = soup.select('.colBoxLeft li, .topicsList li, article')
                    
                    extracted = 0
                    for article in articles[:10]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            if href.startswith('/'):
                                full_url = 'https://www.itmedia.co.jp' + href
                            elif not href.startswith('http'):
                                continue
                            else:
                                full_url = href
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'ITmedia ãƒ“ã‚¸ãƒã‚¹',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            continue
                    
                    print(f"    ITmedia {url.split('/')[-2] if '/' in url else 'main'}: {extracted}ä»¶")
                    
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            return []
    
    def scrape_president_news(self, site_config):
        """ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
        news_list = []
        
        try:
            self.check_rate_limit('president.jp')
            
            # ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®ã‚«ãƒ†ã‚´ãƒª
            president_urls = [
                'https://president.jp/list/category/business',
                'https://president.jp/list/category/money'
            ]
            
            for url in president_urls:
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã®è¨˜äº‹ä¸€è¦§
                    articles = soup.select('.article-list li, .list-item, article')
                    
                    extracted = 0
                    for article in articles[:10]:
                        try:
                            link = article.select_one('a[href]')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if not self.is_social_insurance_relevant(title):
                                continue
                            
                            if href.startswith('/'):
                                full_url = 'https://president.jp' + href
                            elif not href.startswith('http'):
                                continue
                            else:
                                full_url = href
                            
                            category, related_categories, confidence = self.enhanced_categorization(title, '')
                            importance = self.enhanced_importance(title, '')
                            keywords = self.enhanced_keywords(title, '')
                            
                            news_item = {
                                'id': self.generate_id(title, full_url),
                                'title': title,
                                'url': full_url,
                                'source': 'ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³',
                                'category': category,
                                'importance': importance,
                                'summary': self.create_summary(title, category, keywords),
                                'keywords': keywords,
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat(),
                                'related_categories': related_categories,
                                'confidence_score': confidence
                            }
                            
                            news_list.append(news_item)
                            extracted += 1
                            
                        except Exception as e:
                            continue
                    
                    print(f"    ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆ {url.split('/')[-1]}: {extracted}ä»¶")
                    
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            return []
    
    def categorize_news(self, title):
        """ãƒ¬ã‚¬ã‚·ãƒ¼ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆå¾Œæ–¹äº’æ›ï¼‰"""
        category, _, _ = self.enhanced_categorization(title, '')
        return category
    
    def assess_importance(self, title):
        """ãƒ¬ã‚¬ã‚·ãƒ¼é‡è¦åº¦åˆ¤å®šï¼ˆå¾Œæ–¹äº’æ›ï¼‰"""
        return self.enhanced_importance(title, '')
    
    def extract_keywords(self, text):
        """ãƒ¬ã‚¬ã‚·ãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå¾Œæ–¹äº’æ›ï¼‰"""
        return self.enhanced_keywords(text, '')
    
    def generate_daily_report(self, news_data):
        """æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            total_news = len(news_data)
            high_importance = len([n for n in news_data if n.get('importance') == 'é«˜'])
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
            categories = {}
            for news in news_data:
                cat = news.get('category', 'ãã®ä»–')
                categories[cat] = categories.get(cat, 0) + 1
            
            # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            all_keywords = []
            for news in news_data:
                all_keywords.extend(news.get('keywords', []))
            
            keyword_count = {}
            for keyword in all_keywords:
                keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
            
            top_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:10]
            
            report = {
                'generated_at': datetime.now().isoformat(),
                'summary': {
                    'total_news': total_news,
                    'high_importance': high_importance,
                    'categories': list(categories.keys()),
                    'top_keywords': [k[0] for k in top_keywords]
                },
                'categories': categories,
                'keywords': dict(top_keywords)
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            with open(self.report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {total_news}ä»¶")
            return report
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_processed_data(self, news_data):
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            # é‡è¤‡é™¤å»
            unique_news = []
            seen_urls = set()
            
            for news in news_data:
                url = news.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_news.append(news)
            
            # é‡è¦åº¦é †ãƒ»æ–°ã—ã„é †ã§ã‚½ãƒ¼ãƒˆ
            importance_order = {'é«˜': 3, 'ä¸­': 2, 'ä½': 1}
            unique_news.sort(
                key=lambda x: (
                    importance_order.get(x.get('importance', 'ä½'), 1),
                    x.get('scraped_at', '')
                ),
                reverse=True
            )
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
            processed_data = {
                'last_updated': datetime.now().isoformat(),
                'total_count': len(unique_news),
                'news': unique_news,
                'categories': {}
            }
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡
            for news in unique_news:
                category = news.get('category', 'ãã®ä»–')
                if category not in processed_data['categories']:
                    processed_data['categories'][category] = []
                processed_data['categories'][category].append(news)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            with open(self.processed_file, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {len(unique_news)}ä»¶")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_automation(self):
        """è‡ªå‹•åŒ–å®Ÿè¡Œ"""
        try:
            print("ğŸš€ Renderç‰ˆç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–é–‹å§‹")
            print(f"â° é–‹å§‹æ™‚åˆ»: {self.start_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
            print("-" * 60)
            
            # Step 1: ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            all_news = []
            
            # åšåŠ´çœãƒ»å¹´é‡‘æ©Ÿæ§‹ç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹
            mhlw_news = self.scrape_mhlw_comprehensive()
            all_news.extend(mhlw_news)
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç·åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            news_sites_data = self.scrape_news_sites_comprehensive()
            all_news.extend(news_sites_data)
            
            if not all_news:
                print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å¤±æ•— - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
                all_news = [{
                    'title': 'ç¤¾ä¼šä¿é™ºåˆ¶åº¦ã®æœ€æ–°æƒ…å ±ã«ã¤ã„ã¦',
                    'url': 'https://www.mhlw.go.jp',
                    'source': 'åšç”ŸåŠ´åƒçœ',
                    'category': 'ç¤¾ä¼šä¿é™ºå…¨èˆ¬',
                    'importance': 'ä¸­',
                    'summary': 'ç¾åœ¨æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­ã§ã™ã€‚ç¤¾ä¼šä¿é™ºåˆ¶åº¦ã«é–¢ã™ã‚‹é‡è¦ãªå¤‰æ›´ã«ã¤ã„ã¦ã¯åšç”ŸåŠ´åƒçœã®å…¬å¼ã‚µã‚¤ãƒˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚',
                    'keywords': ['ç¤¾ä¼šä¿é™º', 'åˆ¶åº¦å¤‰æ›´', 'åšç”ŸåŠ´åƒçœ'],
                    'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    'scraped_at': datetime.now().isoformat()
                }, {
                    'title': 'å¹´é‡‘åˆ¶åº¦æ”¹æ­£ã®å‹•å‘',
                    'url': 'https://www.nenkin.go.jp',
                    'source': 'æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹',
                    'category': 'åšç”Ÿå¹´é‡‘',
                    'importance': 'é«˜',
                    'summary': 'å¹´é‡‘åˆ¶åº¦ã®æ”¹æ­£å‹•å‘ã«ã¤ã„ã¦ç¶™ç¶šçš„ã«æƒ…å ±åé›†ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚æœ€æ–°æƒ…å ±ã¯æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹ã®å…¬å¼ç™ºè¡¨ã‚’ã”ç¢ºèªãã ã•ã„ã€‚',
                    'keywords': ['å¹´é‡‘', 'åˆ¶åº¦æ”¹æ­£', 'æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹'],
                    'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    'scraped_at': datetime.now().isoformat()
                }]
            
            print(f"ğŸ“¡ ç·åé›†ä»¶æ•°: {len(all_news)}ä»¶")
            
            # é–¢é€£æ€§ã®ä½ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
            relevant_news = []
            for news in all_news:
                if self.is_social_insurance_relevant(news['title']):
                    relevant_news.append(news)
                else:
                    print(f"  ğŸš« é–¢é€£æ€§ä½ãé™¤å¤–: {news['title'][:40]}...")
            
            all_news = relevant_news
            print(f"ğŸ“‹ é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(all_news)}ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰")
            
            # Step 2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ä¿å­˜
            if not self.save_processed_data(all_news):
                print("âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—")
                return False
            
            # Step 3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self.generate_daily_report(all_news)
            
            # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
            end_time = datetime.now()
            duration = end_time - self.start_time
            
            print("-" * 60)
            print("ğŸ‰ è‡ªå‹•åŒ–å®Œäº†!")
            print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration.total_seconds():.1f}ç§’")
            print(f"ğŸ“Š å‡¦ç†ä»¶æ•°: {len(all_news)}ä»¶")
            print(f"ğŸ• å®Œäº†æ™‚åˆ»: {end_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
            
            return True
            
        except Exception as e:
            print(f"âŒ è‡ªå‹•åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print("è©³ç´°ã‚¨ãƒ©ãƒ¼:")
            traceback.print_exc()
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        print("\nğŸ¤– è‡ªå‹•åŒ–å‡¦ç†é–‹å§‹...")
        automation = RenderNewsAutomation()
        success = automation.run_automation()
        
        if success:
            print("\nâœ… ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–æˆåŠŸ!")
            print(f"å‡¦ç†çµ‚äº†æ™‚åˆ»: {datetime.now()}")
            sys.exit(0)
        else:
            print("\nâŒ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–å¤±æ•—!")
            print(f"å‡¦ç†çµ‚äº†æ™‚åˆ»: {datetime.now()}")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nğŸ’¥ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print("ã‚¨ãƒ©ãƒ¼è©³ç´°:")
        traceback.print_exc()
        print(f"å‡¦ç†çµ‚äº†æ™‚åˆ»: {datetime.now()}")
        sys.exit(1)

if __name__ == "__main__":
    main()