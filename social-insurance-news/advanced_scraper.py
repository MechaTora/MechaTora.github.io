#!/usr/bin/env python3
"""
ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ã‚µã‚¤ãƒˆåˆ¥å°‚ç”¨ã‚¯ãƒ©ã‚¹ + çµ±åˆç®¡ç†
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import hashlib
from typing import List, Dict, Optional

class BaseScraper:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_url: str, name: str):
        self.base_url = base_url
        self.name = name
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; SocialInsuranceBot/1.0; +https://github.com/social-insurance-news)'
        })
        
        self.social_keywords = [
            'å¥åº·ä¿é™º', 'åšç”Ÿå¹´é‡‘', 'é›‡ç”¨ä¿é™º', 'åŠ´ç½ä¿é™º', 'ä»‹è­·ä¿é™º',
            'ç¤¾ä¼šä¿é™º', 'ä¿é™ºæ–™', 'å¹´é‡‘', 'çµ¦ä»˜', 'é©ç”¨æ‹¡å¤§', 'åˆ¶åº¦æ”¹æ­£',
            'è¢«ä¿é™ºè€…', 'äº‹æ¥­ä¸»', 'ä¿é™ºè€…', 'å”ä¼šã‘ã‚“ã½', 'å›½æ°‘å¹´é‡‘',
            # è¿½åŠ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            'è€é½¢å¹´é‡‘', 'éšœå®³å¹´é‡‘', 'éºæ—å¹´é‡‘', 'å¤±æ¥­çµ¦ä»˜', 'è‚²å…ä¼‘æ¥­çµ¦ä»˜',
            'ä»‹è­·ä¼‘æ¥­çµ¦ä»˜', 'æ•™è‚²è¨“ç·´çµ¦ä»˜', 'åŠ´åƒç½å®³', 'æ¥­å‹™ç½å®³', 'é€šå‹¤ç½å®³',
            'åŠ´ç½çµ¦ä»˜', 'è¦ä»‹è­·', 'è¦æ”¯æ´', 'ä»‹è­·çµ¦ä»˜', 'ä»‹è­·å ±é…¬',
            'ãƒ¡ãƒªãƒƒãƒˆåˆ¶', 'è¢«æ‰¶é¤Šè€…', 'ä¿é™ºçµ¦ä»˜', 'æœˆæ¬¡ç´ä»˜ç‡', 'æ–½è¡ŒçŠ¶æ³',
            'ç‰¹ä¾‹', 'æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹', 'çµ„åˆå¥ä¿', 'å›½æ°‘å¥åº·ä¿é™º'
        ]
    
    def is_social_insurance_related(self, text: str) -> bool:
        """ç¤¾ä¼šä¿é™ºé–¢é€£åˆ¤å®š"""
        return any(keyword in text for keyword in self.social_keywords)
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # æ”¹è¡Œãƒ»ã‚¿ãƒ–ãƒ»ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹é™¤å»
        text = re.sub(r'\s+', ' ', text)
        # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰
        text = text.replace('&nbsp;', ' ').replace('&amp;', '&')
        return text.strip()
    
    def extract_date(self, text: str) -> Optional[str]:
        """æ—¥ä»˜æŠ½å‡º"""
        patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                if 'ä»¤å’Œ' in pattern:
                    # ä»¤å’Œå¹´ã‚’è¥¿æš¦ã«å¤‰æ›
                    reiwa_year = int(match.group(1))
                    year = 2018 + reiwa_year
                    return f"{year}å¹´{match.group(2)}æœˆ{match.group(3)}æ—¥"
                else:
                    return match.group(0)
        
        return None
    
    def generate_hash(self, url: str, title: str) -> str:
        """è¨˜äº‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        content = f"{url}_{title}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def safe_request(self, url: str, timeout: int = 10) -> Optional[BeautifulSoup]:
        """å®‰å…¨ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"âŒ {self.name} ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None

class MHLWScraper(BaseScraper):
    """åšç”ŸåŠ´åƒçœå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__("https://www.mhlw.go.jp", "åšç”ŸåŠ´åƒçœ")
        # æœˆåˆ¥å ±é“ç™ºè¡¨ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ä½¿ç”¨
        self.monthly_urls = [
            "/stf/houdou/houdou_list_202501.html",  # 2025å¹´1æœˆ
            "/stf/houdou/houdou_list_202412.html",  # 2024å¹´12æœˆ
        ]
    
    def scrape_news(self, days_back: int = 30) -> List[Dict]:
        """æœˆåˆ¥å ±é“ç™ºè¡¨ä¸€è¦§ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print(f"ğŸ›ï¸ {self.name} ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        news_list = []
        
        for monthly_url in self.monthly_urls:
            print(f"  ğŸ“… {monthly_url} ã‚’èª¿æŸ»ä¸­...")
            soup = self.safe_request(self.base_url + monthly_url)
            if not soup:
                continue
            
            # å…¨ãƒªãƒ³ã‚¯ã‹ã‚‰ç¤¾ä¼šä¿é™ºé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                try:
                    title = self.clean_text(link.get_text())
                    href = link.get('href')
                    
                    if not title or not href or len(title) < 10:
                        continue
                    
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                    if not (('/stf/houdou/' in href or '/newpage_' in href) and 
                           '.html' in href and
                           'list' not in href and 
                           'index' not in href):
                        continue
                    
                    # ç¤¾ä¼šä¿é™ºé–¢é€£ãƒ•ã‚£ãƒ«ã‚¿
                    if not self.is_social_insurance_related(title):
                        continue
                    
                    full_url = urljoin(self.base_url, href)
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if any(news.get('url') == full_url for news in news_list):
                        continue
                    
                    # è¨˜äº‹è©³ç´°å–å¾—
                    article_data = self.get_article_details(full_url, title)
                    if article_data:
                        news_list.append(article_data)
                        print(f"  âœ… åé›†: {title[:60]}...")
                    
                    # ç¤¼å„€æ­£ã—ã„é–“éš”
                    time.sleep(1)
                    
                    # åˆ¶é™ï¼ˆ1ã¤ã®æœˆã‹ã‚‰æœ€å¤§10ä»¶ï¼‰
                    monthly_count = sum(1 for n in news_list if monthly_url in str(n.get('scraped_at', '')))
                    if monthly_count >= 10:
                        break
                        
                except Exception as e:
                    print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        print(f"ğŸ›ï¸ {self.name}: {len(news_list)}ä»¶åé›†å®Œäº†")
        return news_list
    
    def get_article_details(self, url: str, title: str) -> Optional[Dict]:
        """è¨˜äº‹è©³ç´°æƒ…å ±å–å¾—"""
        soup = self.safe_request(url)
        if not soup:
            return None
        
        try:
            # æœ¬æ–‡æŠ½å‡º
            content_selectors = [
                '.main-content', '.article-body', '.content-area',
                '#main', '.section-content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = self.clean_text(content_elem.get_text())[:2000]
                    break
            
            # æ—¥ä»˜æŠ½å‡º
            date_text = soup.get_text()
            published_date = self.extract_date(date_text)
            
            return {
                'id': self.generate_hash(url, title),
                'title': title,
                'url': url,
                'content': content,
                'source': self.name,
                'published_date': published_date or datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                'scraped_at': datetime.now().isoformat(),
                'content_length': len(content)
            }
            
        except Exception as e:
            print(f"è¨˜äº‹è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

class NenkinScraper(BaseScraper):
    """æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__("https://www.nenkin.go.jp", "æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹")
        self.news_url = "/oshirase/"
    
    def scrape_news(self, days_back: int = 3) -> List[Dict]:
        """ãŠçŸ¥ã‚‰ã›ä¸€è¦§ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print(f"ğŸ’° {self.name} ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        news_list = []
        
        soup = self.safe_request(self.base_url + self.news_url)
        if not soup:
            return news_list
        
        # ãŠçŸ¥ã‚‰ã›ãƒªã‚¹ãƒˆå–å¾—
        news_items = soup.find_all('li', class_=re.compile(r'news|item'))
        if not news_items:
            # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿
            news_items = soup.find_all('a', href=re.compile(r'/oshirase/'))
        
        for item in news_items[:15]:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯æŠ½å‡º
                link_elem = item.find('a') if item.name != 'a' else item
                if not link_elem:
                    continue
                
                title = self.clean_text(link_elem.get_text())
                if not self.is_social_insurance_related(title):
                    continue
                
                href = link_elem.get('href')
                full_url = urljoin(self.base_url, href)
                
                # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
                article_data = {
                    'id': self.generate_hash(full_url, title),
                    'title': title,
                    'url': full_url,
                    'content': title,  # è©³ç´°ã¯å¾Œã§å–å¾—
                    'source': self.name,
                    'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    'scraped_at': datetime.now().isoformat()
                }
                
                news_list.append(article_data)
                print(f"  âœ… åé›†: {title[:50]}...")
                
                time.sleep(2)
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"ğŸ’° {self.name}: {len(news_list)}ä»¶åé›†å®Œäº†")
        return news_list

class SocialInsuranceNewsCollector:
    """çµ±åˆãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ç®¡ç†"""
    
    def __init__(self):
        self.scrapers = [
            MHLWScraper(),
            NenkinScraper()
        ]
        self.collected_news = []
        self.seen_hashes = set()
    
    def collect_all_news(self) -> List[Dict]:
        """å…¨ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print("ğŸŒ… ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€æ‹¬åé›†é–‹å§‹")
        start_time = datetime.now()
        
        all_news = []
        for scraper in self.scrapers:
            try:
                news_list = scraper.scrape_news()
                
                # é‡è¤‡é™¤å»
                for news in news_list:
                    if news['id'] not in self.seen_hashes:
                        self.seen_hashes.add(news['id'])
                        all_news.append(news)
                        
            except Exception as e:
                print(f"âŒ {scraper.name} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ (æ–°ã—ã„é †)
        all_news.sort(key=lambda x: x['scraped_at'], reverse=True)
        
        duration = (datetime.now() - start_time).seconds
        print(f"ğŸ‰ åé›†å®Œäº†: {len(all_news)}ä»¶ ({duration}ç§’)")
        
        self.collected_news = all_news
        return all_news
    
    def save_raw_data(self, filename: str = 'raw_news_data.json'):
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        output_data = {
            'collection_time': datetime.now().isoformat(),
            'total_count': len(self.collected_news),
            'sources': list(set(news['source'] for news in self.collected_news)),
            'news': self.collected_news
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜: {filename}")
        return output_data

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    collector = SocialInsuranceNewsCollector()
    news_data = collector.collect_all_news()
    collector.save_raw_data()
    
    print(f"\nğŸ“Š åé›†çµæœ:")
    for news in news_data[:3]:
        print(f"- {news['title'][:60]}... ({news['source']})")