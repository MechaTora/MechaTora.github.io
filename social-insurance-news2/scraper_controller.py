#!/usr/bin/env python3
"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åˆ¶å¾¡ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€ãƒ­ã‚°ç®¡ç†
"""

import time
import logging
from datetime import datetime, timedelta
import json
from typing import Dict, List
import traceback

class ScrapingController:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åˆ¶å¾¡ãƒ»ç›£è¦–"""
    
    def __init__(self, config_file: str = 'scraping_config.json'):
        self.config = self.load_config(config_file)
        self.setup_logging()
        self.error_count = {}
        self.request_times = {}
        
    def load_config(self, config_file: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        default_config = {
            "rate_limits": {
                "mhlw.go.jp": 2.0,      # 2ç§’é–“éš”
                "nenkin.go.jp": 2.0,    # 2ç§’é–“éš”
                "kyoukaikenpo.or.jp": 3.0  # 3ç§’é–“éš”
            },
            "retry_config": {
                "max_retries": 3,
                "retry_delay": 5,
                "backoff_factor": 2
            },
            "timeout_config": {
                "connect_timeout": 10,
                "read_timeout": 30
            },
            "error_thresholds": {
                "max_errors_per_site": 5,
                "max_total_errors": 15
            }
        }
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        except FileNotFoundError:
            # ãƒ‡ãƒ•ã‚©ãƒ«è¨­å®šã§è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
                
        return default_config
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraping.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def check_rate_limit(self, domain: str) -> bool:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        now = datetime.now()
        
        if domain in self.request_times:
            last_request = self.request_times[domain]
            required_interval = self.config["rate_limits"].get(domain, 2.0)
            
            elapsed = (now - last_request).total_seconds()
            if elapsed < required_interval:
                wait_time = required_interval - elapsed
                self.logger.info(f"â³ {domain} ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {wait_time:.1f}ç§’å¾…æ©Ÿ")
                time.sleep(wait_time)
        
        self.request_times[domain] = now
        return True
    
    def handle_error(self, site: str, error: Exception, url: str = "") -> bool:
        """ã‚¨ãƒ©ãƒ¼å‡¦ç†"""
        if site not in self.error_count:
            self.error_count[site] = 0
        
        self.error_count[site] += 1
        
        error_msg = f"âŒ {site} ã‚¨ãƒ©ãƒ¼({self.error_count[site]}): {error}"
        if url:
            error_msg += f" - URL: {url}"
            
        self.logger.error(error_msg)
        
        # ã‚¨ãƒ©ãƒ¼é–¾å€¤ãƒã‚§ãƒƒã‚¯
        max_site_errors = self.config["error_thresholds"]["max_errors_per_site"] 
        max_total_errors = self.config["error_thresholds"]["max_total_errors"]
        
        total_errors = sum(self.error_count.values())
        
        if self.error_count[site] >= max_site_errors:
            self.logger.warning(f"âš ï¸ {site} ã‚¨ãƒ©ãƒ¼ä¸Šé™åˆ°é”: ã‚¹ã‚­ãƒƒãƒ—")
            return False
            
        if total_errors >= max_total_errors:
            self.logger.error(f"ğŸš¨ ç·ã‚¨ãƒ©ãƒ¼æ•°ä¸Šé™åˆ°é”: å‡¦ç†ä¸­æ–­")
            return False
            
        return True
    
    def retry_with_backoff(self, func, *args, **kwargs):
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤"""
        max_retries = self.config["retry_config"]["max_retries"]
        base_delay = self.config["retry_config"]["retry_delay"]
        backoff_factor = self.config["retry_config"]["backoff_factor"]
        
        for attempt in range(max_retries + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                if attempt == max_retries:
                    raise e
                
                delay = base_delay * (backoff_factor ** attempt)
                self.logger.info(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries} ({delay}ç§’å¾Œ)")
                time.sleep(delay)
    
    def generate_report(self) -> Dict:
        """å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        total_errors = sum(self.error_count.values())
        
        report = {
            "execution_time": datetime.now().isoformat(),
            "error_summary": {
                "total_errors": total_errors,
                "errors_by_site": dict(self.error_count),
                "success_rate": max(0, 100 - (total_errors * 10))
            },
            "rate_limit_compliance": "OK",
            "recommendations": []
        }
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        if total_errors > 5:
            report["recommendations"].append("ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„ã§ã™ã€‚ã‚µã‚¤ãƒˆæ§‹é€ å¤‰æ›´ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        if any(count > 3 for count in self.error_count.values()):
            report["recommendations"].append("ç‰¹å®šã‚µã‚¤ãƒˆã§ã‚¨ãƒ©ãƒ¼å¤šç™ºã€‚robots.txtãƒ»åˆ©ç”¨è¦ç´„ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„")
        
        return report
    
    def save_report(self, report: Dict, filename: str = 'scraping_report.json'):
        """ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“‹ å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filename}")

# ä½¿ç”¨ä¾‹ã®çµ±åˆã‚¯ãƒ©ã‚¹      
class ControlledScraper:
    """åˆ¶å¾¡æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.controller = ScrapingController()
        
    def controlled_request(self, scraper_func, site_name: str, *args, **kwargs):
        """åˆ¶å¾¡ä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        # ãƒ‰ãƒ¡ã‚¤ãƒ³æŠ½å‡º
        if hasattr(scraper_func, '__self__'):
            domain = scraper_func.__self__.base_url.replace('https://', '').replace('http://', '')
        else:
            domain = site_name
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        self.controller.check_rate_limit(domain)
        
        try:
            # ãƒªãƒˆãƒ©ã‚¤ä»˜ãå®Ÿè¡Œ
            result = self.controller.retry_with_backoff(scraper_func, *args, **kwargs)
            return result
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼å‡¦ç†
            continue_scraping = self.controller.handle_error(site_name, e)
            if not continue_scraping:
                raise Exception(f"ã‚µã‚¤ãƒˆ {site_name} ã§ã‚¨ãƒ©ãƒ¼ä¸Šé™åˆ°é”")
            return None
    
    def run_controlled_collection(self, collector):
        """åˆ¶å¾¡ä»˜ããƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Ÿè¡Œ"""
        try:
            self.controller.logger.info("ğŸŒ… åˆ¶å¾¡ä»˜ããƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹")
            
            # å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆ¶å¾¡ä»˜ãã§å®Ÿè¡Œ
            all_news = []
            for scraper in collector.scrapers:
                try:
                    news_list = self.controlled_request(
                        scraper.scrape_news, 
                        scraper.name
                    )
                    if news_list:
                        all_news.extend(news_list)
                        
                except Exception as e:
                    self.controller.logger.error(f"âŒ {scraper.name} å®Œå…¨å¤±æ•—: {e}")
                    continue
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ä¿å­˜
            report = self.controller.generate_report()
            self.controller.save_report(report)
            
            self.controller.logger.info(f"âœ… åˆ¶å¾¡ä»˜ãåé›†å®Œäº†: {len(all_news)}ä»¶")
            return all_news
            
        except Exception as e:
            self.controller.logger.error(f"ğŸš¨ åé›†ãƒ—ãƒ­ã‚»ã‚¹å®Œå…¨å¤±æ•—: {e}")
            self.controller.logger.error(traceback.format_exc())
            return []

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    from advanced_scraper import SocialInsuranceNewsCollector
    
    collector = SocialInsuranceNewsCollector()
    controlled_scraper = ControlledScraper()
    
    news_data = controlled_scraper.run_controlled_collection(collector)
    print(f"ğŸ“Š æœ€çµ‚çµæœ: {len(news_data)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†")