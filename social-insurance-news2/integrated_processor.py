#!/usr/bin/env python3
"""
çµ±åˆãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° â†’ AIè¦ç´„ â†’ ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ– â†’ HTMLç”Ÿæˆ
"""

import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import hashlib
from dataclasses import dataclass, asdict
from advanced_scraper import SocialInsuranceNewsCollector
from scraper_controller import ControlledScraper
from free_ai_summary import FreeNewsSummarizer
from news_site_scraper import SocialInsuranceNewsAggregator

@dataclass
class ProcessedNews:
    """å‡¦ç†æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹æ§‹é€ """
    id: str
    title: str
    url: str
    source: str
    category: str
    summary: str
    importance: str
    published_date: str
    scraped_at: str
    keywords: List[str]
    related_categories: List[str]
    content_length: int
    confidence_score: float

class EnhancedSummarizer(FreeNewsSummarizer):
    """å¼·åŒ–ç‰ˆAIè¦ç´„ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        super().__init__()
        
        # ã‚ˆã‚Šè©³ç´°ãªã‚«ãƒ†ã‚´ãƒªå®šç¾©
        self.detailed_categories = {
            "å¥åº·ä¿é™º": {
                "keywords": ["å¥åº·ä¿é™º", "å”ä¼šã‘ã‚“ã½", "çµ„åˆå¥ä¿", "åŒ»ç™‚ä¿é™º", "ä¿é™ºæ–™ç‡", "è¢«æ‰¶é¤Šè€…"],
                "subcategories": ["ä¿é™ºæ–™", "çµ¦ä»˜", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "åšç”Ÿå¹´é‡‘": {
                "keywords": ["åšç”Ÿå¹´é‡‘", "è€é½¢å¹´é‡‘", "éšœå®³å¹´é‡‘", "éºæ—å¹´é‡‘", "å¹´é‡‘ä¿é™ºæ–™", "å—çµ¦"],
                "subcategories": ["ä¿é™ºæ–™", "çµ¦ä»˜", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "é›‡ç”¨ä¿é™º": {
                "keywords": ["é›‡ç”¨ä¿é™º", "å¤±æ¥­çµ¦ä»˜", "è‚²å…ä¼‘æ¥­çµ¦ä»˜", "ä»‹è­·ä¼‘æ¥­çµ¦ä»˜", "æ•™è‚²è¨“ç·´çµ¦ä»˜"],
                "subcategories": ["çµ¦ä»˜", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´", "ä¿é™ºæ–™"]
            },
            "åŠ´ç½ä¿é™º": {
                "keywords": ["åŠ´ç½ä¿é™º", "åŠ´åƒç½å®³", "æ¥­å‹™ç½å®³", "é€šå‹¤ç½å®³", "åŠ´ç½çµ¦ä»˜"],
                "subcategories": ["çµ¦ä»˜", "èªå®š", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "ä»‹è­·ä¿é™º": {
                "keywords": ["ä»‹è­·ä¿é™º", "è¦ä»‹è­·", "è¦æ”¯æ´", "ä»‹è­·çµ¦ä»˜", "ä»‹è­·å ±é…¬"],
                "subcategories": ["çµ¦ä»˜", "èªå®š", "æ‰‹ç¶šã", "åˆ¶åº¦å¤‰æ›´"]
            },
            "ç¤¾ä¼šä¿é™ºå…¨èˆ¬": {
                "keywords": ["ç¤¾ä¼šä¿é™º", "é©ç”¨æ‹¡å¤§", "è¢«ä¿é™ºè€…", "äº‹æ¥­ä¸»", "åˆ¶åº¦æ”¹æ­£", "æ³•æ”¹æ­£"],
                "subcategories": ["åˆ¶åº¦å¤‰æ›´", "é©ç”¨", "æ‰‹ç¶šã", "æ³•ä»¤"]
            }
        }
        
        # é‡è¦åº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.importance_keywords = {
            "é«˜": ["æ³•æ”¹æ­£", "åˆ¶åº¦æ”¹æ­£", "æ–°è¨­", "å»ƒæ­¢", "æ–½è¡Œ", "å…¬å¸ƒ", "çœä»¤", "å‘Šç¤º"],
            "ä¸­": ["æ”¹å®š", "å¤‰æ›´", "è¦‹ç›´ã—", "æ‹¡å¤§", "ç¸®å°", "æ–™ç‡å¤‰æ›´", "åŸºæº–å¤‰æ›´"],
            "ä½": ["æ‰‹ç¶šã", "æ§˜å¼", "ãŠçŸ¥ã‚‰ã›", "æ¡ˆå†…", "èª¬æ˜ä¼š", "ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆ"]
        }
    
    def enhanced_categorization(self, title: str, content: str) -> Tuple[str, List[str], float]:
        """å¼·åŒ–ç‰ˆã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        text = title + " " + content
        
        category_scores = {}
        related_categories = []
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        for category, data in self.detailed_categories.items():
            score = 0
            matched_keywords = []
            
            for keyword in data["keywords"]:
                count = text.count(keyword)
                if count > 0:
                    score += count * (len(keyword) / 3)  # é•·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã»ã©é‡è¦
                    matched_keywords.append(keyword)
            
            if score > 0:
                category_scores[category] = {
                    "score": score,
                    "keywords": matched_keywords
                }
        
        if not category_scores:
            return "ãã®ä»–", [], 0.0
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’ä¸»ã‚«ãƒ†ã‚´ãƒªã«
        primary_category = max(category_scores.keys(), key=lambda k: category_scores[k]["score"])
        confidence = min(category_scores[primary_category]["score"] / 10, 1.0)
        
        # é–¢é€£ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¹ã‚³ã‚¢0.3ä»¥ä¸Šï¼‰
        for cat, data in category_scores.items():
            if cat != primary_category and data["score"] >= 1.0:
                related_categories.append(cat)
        
        return primary_category, related_categories, confidence
    
    def determine_importance(self, title: str, content: str) -> str:
        """é‡è¦åº¦åˆ¤å®š"""
        text = title + " " + content
        
        # é«˜é‡è¦åº¦ãƒã‚§ãƒƒã‚¯
        for keyword in self.importance_keywords["é«˜"]:
            if keyword in text:
                return "é«˜"
        
        # ä¸­é‡è¦åº¦ãƒã‚§ãƒƒã‚¯
        for keyword in self.importance_keywords["ä¸­"]:
            if keyword in text:
                return "ä¸­"
        
        return "ä½"
    
    def extract_enhanced_keywords(self, title: str, content: str) -> List[str]:
        """å¼·åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        text = title + " " + content
        keywords = []
        
        # åˆ¶åº¦åãƒ»çµ„ç¹”åæŠ½å‡º
        institution_patterns = [
            r'å”ä¼šã‘ã‚“ã½', r'å¥åº·ä¿é™ºçµ„åˆ', r'å¹´é‡‘äº‹å‹™æ‰€', r'å¹´é‡‘æ©Ÿæ§‹',
            r'åšç”ŸåŠ´åƒçœ', r'ç¤¾ä¼šä¿é™ºåŠ´å‹™å£«', r'åŠ´åƒåŸºæº–ç›£ç£ç½²'
        ]
        
        for pattern in institution_patterns:
            if re.search(pattern, text):
                keywords.append(pattern)
        
        # é‡‘é¡ãƒ»ç‡ãƒ»æœŸé–“æŠ½å‡º
        numeric_patterns = [
            r'\d+\.?\d*%',  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ
            r'\d+å††',       # é‡‘é¡
            r'\d+ä¸‡å††',     # ä¸‡å††å˜ä½
            r'\d+å¹´\d+æœˆ',  # å¹´æœˆ
        ]
        
        for pattern in numeric_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches[:3])  # æœ€å¤§3å€‹ã¾ã§
        
        return list(set(keywords))[:10]  # é‡è¤‡é™¤å»ãƒ»ä¸Šä½10å€‹
    
    def generate_enhanced_summary(self, news_data: Dict) -> ProcessedNews:
        """å¼·åŒ–ç‰ˆè¦ç´„ç”Ÿæˆ"""
        title = news_data.get('title', '')
        content = news_data.get('content', '')
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
        category, related_categories, confidence = self.enhanced_categorization(title, content)
        
        # é‡è¦åº¦åˆ¤å®š  
        importance = self.determine_importance(title, content)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = self.extract_enhanced_keywords(title, content)
        
        # è¦ç´„æ–‡ç”Ÿæˆï¼ˆã‚ˆã‚Šè©³ç´°ã«ï¼‰
        summary = self.create_detailed_summary(title, content, category, keywords)
        
        return ProcessedNews(
            id=news_data.get('id', ''),
            title=title,
            url=news_data.get('url', ''),
            source=news_data.get('source', ''),
            category=category,
            summary=summary,
            importance=importance,
            published_date=news_data.get('published_date', ''),
            scraped_at=news_data.get('scraped_at', ''),
            keywords=keywords,
            related_categories=related_categories,
            content_length=len(content),
            confidence_score=confidence
        )
    
    def create_detailed_summary(self, title: str, content: str, category: str, keywords: List[str]) -> str:
        """è©³ç´°è¦ç´„æ–‡ä½œæˆ"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦éƒ¨åˆ†æŠ½å‡º
        title_clean = re.sub(r'ã«ã¤ã„ã¦$|ã«é–¢ã—ã¦$|ã®ãŠçŸ¥ã‚‰ã›$', '', title)
        
        summary_parts = []
        
        # ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤º
        summary_parts.append(f"ã€{category}ã€‘")
        
        # ä¸»è¦å†…å®¹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        if len(title_clean) > 20:
            title_summary = title_clean[:30] + "..."
        else:
            title_summary = title_clean
        summary_parts.append(title_summary)
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ 
        if keywords:
            key_info = []
            for keyword in keywords[:3]:
                if '%' in keyword or 'å††' in keyword or 'å¹´' in keyword:
                    key_info.append(keyword)
            
            if key_info:
                summary_parts.append(f"({', '.join(key_info)})")
        
        # å®Ÿæ–½æ™‚æœŸæŠ½å‡º
        date_match = re.search(r'(\d{4}å¹´\d{1,2}æœˆ|\d{1,2}æœˆ\d{1,2}æ—¥|ä»¤å’Œ\d+å¹´)', content)
        if date_match:
            summary_parts.append(f"[{date_match.group(1)}]")
        
        summary = " ".join(summary_parts)
        return summary[:120]  # 120æ–‡å­—åˆ¶é™

class IntegratedNewsProcessor:
    """çµ±åˆãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.collector = SocialInsuranceNewsCollector()
        self.controller = ControlledScraper() 
        self.news_aggregator = SocialInsuranceNewsAggregator()
        self.summarizer = EnhancedSummarizer()
        self.processed_news = []
    
    def run_full_pipeline(self) -> List[ProcessedNews]:
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆæ”¿åºœã‚µã‚¤ãƒˆ + ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆçµ±åˆï¼‰"""
        print("ğŸš€ çµ±åˆãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        
        # 1. æ”¿åºœã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print("ğŸ›ï¸ æ”¿åºœã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
        gov_news = self.controller.run_controlled_collection(self.collector)
        
        # 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰ç¤¾ä¼šä¿é™ºæƒ…å ±åé›†ä¸­...")
        news_sites_data = []
        try:
            news_sites_data = self.news_aggregator.collect_all_news()
        except Exception as e:
            print(f"âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        all_raw_news = (gov_news or []) + (news_sites_data or [])
        
        if not all_raw_news:
            print("âŒ å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã«å¤±æ•—")
            return []
        
        print(f"âœ… åˆè¨ˆ{len(all_raw_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆæ”¿åºœ:{len(gov_news or [])}ä»¶ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹:{len(news_sites_data or [])}ä»¶ï¼‰")
        
        # 4. AIè¦ç´„ãƒ»åˆ†é¡å‡¦ç†
        print("ğŸ¤– AIè¦ç´„å‡¦ç†ä¸­...")
        processed_news = []
        
        for news in all_raw_news:
            try:
                processed = self.summarizer.generate_enhanced_summary(news)
                processed_news.append(processed)
                print(f"  âœ… å‡¦ç†å®Œäº†: {processed.title[:40]}...")
                
            except Exception as e:
                print(f"  âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"ğŸ‰ {len(processed_news)}ä»¶ã®è¦ç´„ç”Ÿæˆå®Œäº†")
        
        # 5. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.save_processed_data(processed_news)
        
        self.processed_news = processed_news
        return processed_news
    
    def save_processed_data(self, processed_news: List[ProcessedNews]):
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        structured_data = {
            "last_updated": datetime.now().isoformat(),
            "total_count": len(processed_news),
            "categories": self.group_by_category(processed_news),
            "importance_distribution": self.get_importance_stats(processed_news),
            "news": [asdict(news) for news in processed_news]
        }
        
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        with open('processed_news.json', 'w', encoding='utf-8') as f:
            json.dump(structured_data, f, ensure_ascii=False, indent=2)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«
        for category, news_list in structured_data["categories"].items():
            filename = f'category_{category.replace("/", "_")}.json'
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    "category": category,
                    "count": len(news_list),
                    "news": news_list
                }, f, ensure_ascii=False, indent=2)
        
        print("ğŸ’¾ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
    
    def group_by_category(self, news_list: List[ProcessedNews]) -> Dict:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        categories = {}
        for news in news_list:
            category = news.category
            if category not in categories:
                categories[category] = []
            categories[category].append(asdict(news))
        return categories
    
    def get_importance_stats(self, news_list: List[ProcessedNews]) -> Dict:
        """é‡è¦åº¦çµ±è¨ˆ"""
        importance_count = {"é«˜": 0, "ä¸­": 0, "ä½": 0}
        for news in news_list:
            importance_count[news.importance] += 1
        return importance_count
    
    def generate_daily_report(self) -> Dict:
        """æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.processed_news:
            return {}
        
        report = {
            "date": datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
            "summary": {
                "total_news": len(self.processed_news),
                "high_importance": len([n for n in self.processed_news if n.importance == "é«˜"]),
                "categories": list(set(n.category for n in self.processed_news)),
                "top_keywords": self.get_top_keywords()
            },
            "highlights": [
                asdict(news) for news in self.processed_news 
                if news.importance == "é«˜"
            ][:5]
        }
        
        with open('daily_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report
    
    def get_top_keywords(self) -> List[str]:
        """é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        all_keywords = []
        for news in self.processed_news:
            all_keywords.extend(news.keywords)
        
        keyword_count = {}
        for keyword in all_keywords:
            keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        return sorted(keyword_count.keys(), key=keyword_count.get, reverse=True)[:10]

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    processor = IntegratedNewsProcessor()
    results = processor.run_full_pipeline()
    
    if results:
        report = processor.generate_daily_report()
        print(f"\nğŸ“Š å‡¦ç†çµæœ: {len(results)}ä»¶")
        print(f"ğŸ“ˆ é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹: {report['summary']['high_importance']}ä»¶")
        print(f"ğŸ“‹ ã‚«ãƒ†ã‚´ãƒª: {', '.join(report['summary']['categories'])}")
    else:
        print("âŒ å‡¦ç†å¤±æ•—")