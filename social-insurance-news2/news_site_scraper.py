#!/usr/bin/env python3
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç¤¾ä¼šä¿é™ºæƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
NHKã€æœæ—¥æ–°èã€æ—¥çµŒæ–°èãªã©ã‹ã‚‰ç¤¾ä¼šä¿é™ºé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
"""

import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import hashlib

class NewsBaseScraper:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_url: str, name: str):
        self.base_url = base_url
        self.name = name
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # å¼·åŒ–ã•ã‚ŒãŸç¤¾ä¼šä¿é™ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.social_insurance_keywords = [
            # åŸºæœ¬åˆ¶åº¦
            'ç¤¾ä¼šä¿é™º', 'å¥åº·ä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å›½æ°‘å¹´é‡‘', 'é›‡ç”¨ä¿é™º', 'åŠ´ç½ä¿é™º', 'ä»‹è­·ä¿é™º',
            
            # å…·ä½“çš„åˆ¶åº¦ãƒ»ç”¨èª
            'å”ä¼šã‘ã‚“ã½', 'çµ„åˆå¥ä¿', 'å›½æ°‘å¥åº·ä¿é™º', 'å¾ŒæœŸé«˜é½¢è€…åŒ»ç™‚',
            'è€é½¢å¹´é‡‘', 'éšœå®³å¹´é‡‘', 'éºæ—å¹´é‡‘', 'åŸºç¤å¹´é‡‘', 'åšç”Ÿå¹´é‡‘åŸºé‡‘',
            'å¤±æ¥­çµ¦ä»˜', 'è‚²å…ä¼‘æ¥­çµ¦ä»˜', 'ä»‹è­·ä¼‘æ¥­çµ¦ä»˜', 'æ•™è‚²è¨“ç·´çµ¦ä»˜',
            'åŠ´åƒç½å®³', 'æ¥­å‹™ç½å®³', 'é€šå‹¤ç½å®³', 'åŠ´ç½èªå®š',
            'è¦ä»‹è­·èªå®š', 'ä»‹è­·ã‚µãƒ¼ãƒ“ã‚¹', 'ä»‹è­·å ±é…¬',
            
            # ä¿é™ºæ–™ãƒ»çµ¦ä»˜é–¢é€£
            'ä¿é™ºæ–™ç‡', 'ä¿é™ºæ–™æ”¹å®š', 'æ¨™æº–å ±é…¬', 'è³ä¸', 'ä¿é™ºçµ¦ä»˜',
            'åŒ»ç™‚è²»', 'çª“å£è² æ‹…', 'é«˜é¡ç™‚é¤Šè²»', 'å‚·ç—…æ‰‹å½“é‡‘',
            'å‡ºç”£è‚²å…ä¸€æ™‚é‡‘', 'å‡ºç”£æ‰‹å½“é‡‘', 'å…ç«¥æ‰‹å½“',
            
            # åˆ¶åº¦å¤‰æ›´ãƒ»æ”¿ç­–
            'åˆ¶åº¦æ”¹æ­£', 'æ³•æ”¹æ­£', 'é©ç”¨æ‹¡å¤§', 'åˆ¶åº¦è¦‹ç›´ã—',
            'ç¤¾ä¼šä¿éšœ', 'åŒ»ç™‚åˆ¶åº¦æ”¹é©', 'å¹´é‡‘åˆ¶åº¦æ”¹é©',
            
            # çµ„ç¹”ãƒ»æ©Ÿé–¢
            'åšç”ŸåŠ´åƒçœ', 'æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹', 'å…¨å›½å¥åº·ä¿é™ºå”ä¼š',
            'å¥åº·ä¿é™ºçµ„åˆ', 'å›½æ°‘å¥åº·ä¿é™ºçµ„åˆ', 'å”ä¼šã‘ã‚“ã½',
            
            # é–¢é€£ç”¨èª
            'è¢«ä¿é™ºè€…', 'è¢«æ‰¶é¤Šè€…', 'ä¿é™ºè€…', 'äº‹æ¥­ä¸»', 'çµ¦ä¸æ‰€å¾—è€…',
            'ãƒã‚¤ãƒŠãƒ³ãƒãƒ¼ã‚«ãƒ¼ãƒ‰', 'ãƒã‚¤ãƒŠä¿é™ºè¨¼', 'å¥åº·ä¿é™ºè¨¼'
        ]
    
    def is_social_insurance_related(self, text: str) -> bool:
        """ç¤¾ä¼šä¿é™ºé–¢é€£åˆ¤å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        text_lower = text.lower()
        return any(keyword in text for keyword in self.social_insurance_keywords)
    
    def safe_request(self, url: str, timeout: int = 10) -> Optional[BeautifulSoup]:
        """å®‰å…¨ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"âŒ {self.name} ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('&nbsp;', ' ').replace('&amp;', '&')
        return text.strip()
    
    def extract_date_from_text(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥ä»˜æŠ½å‡º"""
        patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{1,2})æœˆ(\d{1,2})æ—¥',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                if len(match.groups()) == 3:
                    return f"{match.group(1)}å¹´{match.group(2)}æœˆ{match.group(3)}æ—¥"
                elif len(match.groups()) == 2:
                    current_year = datetime.now().year
                    return f"{current_year}å¹´{match.group(1)}æœˆ{match.group(2)}æ—¥"
        
        return None
    
    def generate_news_id(self, url: str, title: str) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹IDç”Ÿæˆ"""
        content = f"{url}_{title}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

class NHKScraper(NewsBaseScraper):
    """NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ç¤¾ä¼šä¿é™ºã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__("https://www3.nhk.or.jp", "NHKãƒ‹ãƒ¥ãƒ¼ã‚¹")
        self.search_urls = [
            "/news/cat08.html",  # æ”¿æ²»
            "/news/cat05.html",  # çµŒæ¸ˆ
            "/news/cat07.html"   # æš®ã‚‰ã—
        ]
    
    def scrape_news(self, days_back: int = 7) -> List[Dict]:
        """NHKãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print(f"ğŸ“º {self.name} ç¤¾ä¼šä¿é™ºé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        news_list = []
        
        for search_url in self.search_urls:
            full_url = self.base_url + search_url
            soup = self.safe_request(full_url)
            
            if not soup:
                continue
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            news_links = soup.find_all('a', href=re.compile(r'/news/html/\d+/'))
            
            for link in news_links[:20]:  # æœ€æ–°20ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                try:
                    title = self.clean_text(link.get_text())
                    href = link.get('href')
                    
                    if not title or not href:
                        continue
                    
                    # ç¤¾ä¼šä¿é™ºé–¢é€£ã‹ãƒã‚§ãƒƒã‚¯
                    if not self.is_social_insurance_related(title):
                        continue
                    
                    full_article_url = urljoin(self.base_url, href)
                    
                    # è¨˜äº‹è©³ç´°å–å¾—
                    article_data = self.get_article_details(full_article_url, title)
                    if article_data:
                        news_list.append(article_data)
                        print(f"  âœ… NHKåé›†: {title[:50]}...")
                    
                    time.sleep(1)  # ç¤¼å„€æ­£ã—ã„é–“éš”
                    
                except Exception as e:
                    print(f"  âŒ NHKã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        print(f"ğŸ“º {self.name}: {len(news_list)}ä»¶åé›†å®Œäº†")
        return news_list
    
    def get_article_details(self, url: str, title: str) -> Optional[Dict]:
        """NHKè¨˜äº‹è©³ç´°å–å¾—"""
        soup = self.safe_request(url)
        if not soup:
            return None
        
        # è¨˜äº‹æœ¬æ–‡å–å¾—
        content_elem = soup.find('div', class_='content--detail-body') or soup.find('div', class_='content')
        content = self.clean_text(content_elem.get_text()) if content_elem else ""
        
        # æ—¥ä»˜å–å¾—
        date_elem = soup.find('time') or soup.find('div', class_='content--date')
        published_date = self.extract_date_from_text(date_elem.get_text()) if date_elem else datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        
        return {
            'id': self.generate_news_id(url, title),
            'title': title,
            'url': url,
            'content': content,
            'source': self.name,
            'published_date': published_date,
            'scraped_at': datetime.now().isoformat(),
            'content_length': len(content)
        }

class YahooNewsScraper(NewsBaseScraper):
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç¤¾ä¼šä¿é™ºã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__("https://news.yahoo.co.jp", "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        self.search_keywords = [
            "ç¤¾ä¼šä¿é™º", "å¹´é‡‘", "å¥åº·ä¿é™º", "é›‡ç”¨ä¿é™º", "ä»‹è­·ä¿é™º"
        ]
    
    def scrape_news(self, days_back: int = 7) -> List[Dict]:
        """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print(f"ğŸŒ {self.name} ç¤¾ä¼šä¿é™ºé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        news_list = []
        
        for keyword in self.search_keywords:
            search_url = f"{self.base_url}/search?p={keyword}&ei=UTF-8"
            soup = self.safe_request(search_url)
            
            if not soup:
                continue
            
            # æ¤œç´¢çµæœã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            news_links = soup.find_all('a', href=re.compile(r'/articles/'))
            
            for link in news_links[:10]:  # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10ä»¶ã¾ã§
                try:
                    title = self.clean_text(link.get_text())
                    href = link.get('href')
                    
                    if not title or not href:
                        continue
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if any(news.get('url') == href for news in news_list):
                        continue
                    
                    full_article_url = urljoin(self.base_url, href)
                    
                    # è¨˜äº‹è©³ç´°å–å¾—
                    article_data = self.get_article_details(full_article_url, title)
                    if article_data:
                        news_list.append(article_data)
                        print(f"  âœ… Yahooåé›†: {title[:50]}...")
                    
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"  âŒ Yahooã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            time.sleep(2)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®é–“éš”
        
        print(f"ğŸŒ {self.name}: {len(news_list)}ä»¶åé›†å®Œäº†")
        return news_list
    
    def get_article_details(self, url: str, title: str) -> Optional[Dict]:
        """Yahoo!è¨˜äº‹è©³ç´°å–å¾—"""
        soup = self.safe_request(url)
        if not soup:
            return None
        
        # è¨˜äº‹æœ¬æ–‡å–å¾—
        content_elem = soup.find('div', class_='articleBody') or soup.find('div', class_='article_body')
        content = self.clean_text(content_elem.get_text()) if content_elem else ""
        
        # æ—¥ä»˜å–å¾—
        date_elem = soup.find('time') or soup.find('span', class_='source')
        published_date = self.extract_date_from_text(date_elem.get_text()) if date_elem else datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        
        return {
            'id': self.generate_news_id(url, title),
            'title': title,
            'url': url,
            'content': content,
            'source': self.name,
            'published_date': published_date,
            'scraped_at': datetime.now().isoformat(),
            'content_length': len(content)
        }

class SocialInsuranceNewsAggregator:
    """ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆåé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.scrapers = [
            NHKScraper(),
            YahooNewsScraper(),
        ]
    
    def collect_all_news(self) -> List[Dict]:
        """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰åé›†"""
        print("ğŸŒŸ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆåé›†é–‹å§‹")
        all_news = []
        
        for scraper in self.scrapers:
            try:
                news = scraper.scrape_news()
                all_news.extend(news)
                time.sleep(3)  # ã‚µã‚¤ãƒˆé–“ã®ç¤¼å„€æ­£ã—ã„é–“éš”
                
            except Exception as e:
                print(f"âŒ {scraper.name} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # é‡è¤‡é™¤å»
        unique_news = []
        seen_urls = set()
        
        for news in all_news:
            url = news.get('url', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_news.append(news)
        
        print(f"ğŸ‰ çµ±åˆåé›†å®Œäº†: {len(unique_news)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        return unique_news

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    aggregator = SocialInsuranceNewsAggregator()
    news_results = aggregator.collect_all_news()
    
    print(f"\nğŸ“Š åé›†çµæœ:")
    for i, news in enumerate(news_results[:10]):
        print(f"{i+1}. {news['title'][:60]}... ({news['source']})")
    
    # çµæœã‚’JSONã§ä¿å­˜
    import json
    with open('news_sites_data.json', 'w', encoding='utf-8') as f:
        json.dump({
            'collection_time': datetime.now().isoformat(),
            'total_count': len(news_results),
            'news': news_results
        }, f, ensure_ascii=False, indent=2)