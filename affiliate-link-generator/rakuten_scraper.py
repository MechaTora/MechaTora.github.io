#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¥½å¤©å•†å“æ¤œç´¢ãƒ»ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
requests + BeautifulSoupã‚’ä½¿ç”¨ã—ãŸæ¥½å¤©å•†å“æ¤œç´¢
"""

import time
import re
import urllib.parse
from typing import Optional, Dict, List
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class RakutenScraper:
    def __init__(self, affiliate_id: str, api_key: str = "", search_interval: float = 2.0):
        """
        æ¥½å¤©æ¤œç´¢ã‚¹ã‚¯ãƒ¬ãƒ¼ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            affiliate_id (str): æ¥½å¤©ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆID
            api_key (str): æ¥½å¤©API ã‚­ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            search_interval (float): æ¤œç´¢é–“éš”ï¼ˆç§’ï¼‰
        """
        self.affiliate_id = affiliate_id
        self.api_key = api_key
        self.search_interval = search_interval
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®šï¼ˆurllib3ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§å¯¾å¿œï¼‰
        try:
            # urllib3 v2.0ä»¥é™ã®æ–°ã—ã„API
            retry_strategy = Retry(
                total=3,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["HEAD", "GET", "OPTIONS"],
                backoff_factor=1
            )
        except TypeError:
            try:
                # urllib3 v1.x ã®å¤ã„API
                retry_strategy = Retry(
                    total=3,
                    status_forcelist=[429, 500, 502, 503, 504],
                    method_whitelist=["HEAD", "GET", "OPTIONS"],
                    backoff_factor=1
                )
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒˆãƒ©ã‚¤è¨­å®š
                print(f"âš ï¸ ãƒªãƒˆãƒ©ã‚¤è¨­å®šã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬è¨­å®šã‚’ä½¿ç”¨: {str(e)}")
                retry_strategy = Retry(
                    total=3,
                    backoff_factor=1
                )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                          '(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        self.timeout = 30
        
        print(f"âœ… æ¥½å¤©ã‚¹ã‚¯ãƒ¬ãƒ¼ãƒ‘ãƒ¼åˆæœŸåŒ–å®Œäº† (ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆID: {affiliate_id[:10]}...)")
    
    def search_product(self, product_name: str, max_results: int = 5) -> List[Dict[str, str]]:
        """
        å•†å“ã‚’æ¤œç´¢ã—ã¦ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰ã¨URLã‚’å–å¾—
        
        Args:
            product_name (str): æ¤œç´¢ã™ã‚‹å•†å“å
            max_results (int): æœ€å¤§å–å¾—ä»¶æ•°
            
        Returns:
            List[Dict]: å•†å“æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        try:
            print(f"ğŸ” æ¥½å¤©æ¤œç´¢ä¸­: {product_name}")
            
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã®æº–å‚™
            encoded_query = urllib.parse.quote_plus(product_name)
            search_url = f"https://search.rakuten.co.jp/search/mall/{encoded_query}/"
            
            # æ¤œç´¢å®Ÿè¡Œ
            response = self.session.get(search_url, timeout=self.timeout)
            response.raise_for_status()
            
            if response.status_code != 200:
                print(f"âŒ æ¥½å¤©æ¤œç´¢ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
                return []
            
            # HTMLãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å•†å“è¦ç´ ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œï¼‰
            product_elements = []
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ¡ã‚¤ãƒ³å•†å“ã‚¨ãƒªã‚¢
            products_1 = soup.select('div[data-iteam-info]')
            product_elements.extend(products_1)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å•†å“ãƒªã‚¹ãƒˆ
            if not product_elements:
                products_2 = soup.select('.searchresultitem')
                product_elements.extend(products_2)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å•†å“ã‚«ãƒ¼ãƒ‰
            if not product_elements:
                products_3 = soup.select('[data-testid="item"]')
                product_elements.extend(products_3)
                
            # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            if not product_elements:
                products_4 = soup.select('div[class*="item"]')
                product_elements.extend(products_4[:20])  # å¤šã™ãã‚‹å ´åˆã¯åˆ¶é™
            
            if not product_elements:
                print(f"âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {product_name}")
                return []
            
            print(f"ğŸ” {len(product_elements)}å€‹ã®å•†å“è¦ç´ ã‚’ç™ºè¦‹")
            
            products = []
            processed_count = 0
            
            for element in product_elements:
                try:
                    if processed_count >= max_results:
                        break
                    
                    # å•†å“ãƒªãƒ³ã‚¯ã®å–å¾—
                    link_element = element.find('a', href=True)
                    if not link_element:
                        continue
                    
                    product_url = link_element.get('href')
                    if not product_url:
                        continue
                    
                    # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                    if product_url.startswith('/'):
                        product_url = 'https://item.rakuten.co.jp' + product_url
                    elif not product_url.startswith('http'):
                        continue
                    
                    # æ¥½å¤©å•†å“URLã®æ¤œè¨¼
                    if 'item.rakuten.co.jp' not in product_url:
                        continue
                    
                    # å•†å“ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                    title = ""
                    title_selectors = [
                        link_element.get('title'),
                        link_element.find('img', alt=True),
                        element.find(class_=re.compile(r'title|name')),
                        element.find('h2'),
                        element.find('h3')
                    ]
                    
                    for selector in title_selectors:
                        if selector:
                            if hasattr(selector, 'get_text'):
                                title = selector.get_text(strip=True)
                            elif hasattr(selector, 'get'):
                                title = selector.get('alt', '') or selector.get('title', '')
                            else:
                                title = str(selector).strip()
                            
                            if title:
                                break
                    
                    if not title:
                        title = "å•†å“åä¸æ˜"
                    
                    # ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰æŠ½å‡º
                    item_code = self._extract_item_code(product_url)
                    
                    # ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ç”Ÿæˆ
                    affiliate_url = self._generate_affiliate_link(product_url, item_code)
                    
                    products.append({
                        'item_code': item_code,
                        'title': title[:100],  # ã‚¿ã‚¤ãƒˆãƒ«é•·åˆ¶é™
                        'url': product_url,
                        'affiliate_url': affiliate_url
                    })
                    processed_count += 1
                    
                except Exception as e:
                    # å€‹åˆ¥å•†å“ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼ã¯ç¶™ç¶š
                    continue
            
            print(f"âœ… æ¥½å¤©æ¤œç´¢å®Œäº†: {len(products)}ä»¶ã®å•†å“ã‚’å–å¾—")
            
            # æ¤œç´¢é–“éš”ã®å¾…æ©Ÿ
            if self.search_interval > 0:
                time.sleep(self.search_interval)
            
            return products
            
        except requests.RequestException as e:
            print(f"âŒ æ¥½å¤©æ¤œç´¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
        except Exception as e:
            print(f"âŒ æ¥½å¤©æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _extract_item_code(self, url: str) -> str:
        """
        å•†å“URLã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            url (str): å•†å“URL
            
        Returns:
            str: ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰
        """
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: /shop/store_id/item_code/
            pattern1 = re.search(r'item\.rakuten\.co\.jp/([^/]+)/([^/?]+)', url)
            if pattern1:
                shop_id = pattern1.group(1)
                item_id = pattern1.group(2)
                return f"{shop_id}:{item_id}"
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
            if '?' in url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                
                # ã‚ˆãã‚ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å
                for param_name in ['iid', 'item_id', 'item']:
                    if param_name in params and params[param_name]:
                        return params[param_name][0]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: URLãƒ‘ã‚¹ã‹ã‚‰æ¨æ¸¬
            path_parts = url.split('/')
            if len(path_parts) >= 2:
                for i, part in enumerate(path_parts):
                    if 'item.rakuten.co.jp' in part and i + 2 < len(path_parts):
                        return f"{path_parts[i+1]}:{path_parts[i+2]}"
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯URLå…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
            import hashlib
            return hashlib.md5(url.encode()).hexdigest()[:16]
            
        except Exception as e:
            print(f"âš ï¸ ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return "unknown"
    
    def _generate_affiliate_link(self, original_url: str, item_code: str) -> str:
        """
        ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
        
        Args:
            original_url (str): å…ƒã®URL
            item_code (str): ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰
            
        Returns:
            str: ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯
        """
        try:
            # æ—¢å­˜ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒ
            from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
            
            parsed = urlparse(original_url)
            params = parse_qs(parsed.query)
            
            # ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            params['iasid'] = [self.affiliate_id]
            
            # ä»–ã®æœ‰ç”¨ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚è¿½åŠ å¯èƒ½
            # params['icm'] = ['1']  # æˆæœå ±é…¬å‹
            
            # URLå†æ§‹ç¯‰
            new_query = urlencode(params, doseq=True)
            new_parsed = parsed._replace(query=new_query)
            affiliate_url = urlunparse(new_parsed)
            
            return affiliate_url
            
        except Exception as e:
            print(f"âŒ æ¥½å¤©ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®URLã«ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDã‚’å˜ç´”è¿½åŠ 
            separator = '&' if '?' in original_url else '?'
            return f"{original_url}{separator}iasid={self.affiliate_id}"
    
    def get_best_match(self, product_name: str) -> Optional[Dict[str, str]]:
        """
        æœ€é©ãªå•†å“ã‚’1ä»¶å–å¾—
        
        Args:
            product_name (str): å•†å“å
            
        Returns:
            Optional[Dict]: å•†å“æƒ…å ±ã¾ãŸã¯None
        """
        products = self.search_product(product_name, max_results=1)
        
        if products:
            return products[0]
        else:
            return None
    
    def test_connection(self) -> bool:
        """
        æ¥½å¤©æ¥ç¶šãƒ†ã‚¹ãƒˆ
        
        Returns:
            bool: æ¥ç¶šæˆåŠŸæ™‚True
        """
        try:
            print("ğŸ” æ¥½å¤©æ¥ç¶šãƒ†ã‚¹ãƒˆä¸­...")
            response = self.session.get("https://www.rakuten.co.jp", timeout=self.timeout)
            response.raise_for_status()
            
            if response.status_code == 200:
                print("âœ… æ¥½å¤©æ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ")
                return True
            else:
                print(f"âŒ æ¥½å¤©æ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ æ¥½å¤©æ¥ç¶šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.session:
                self.session.close()
                print("âœ… æ¥½å¤©ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ æ¥½å¤©ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def __enter__(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ - é–‹å§‹"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ - çµ‚äº†"""
        self.close()


def test_rakuten_scraper():
    """æ¥½å¤© Scraperã®ãƒ†ã‚¹ãƒˆ"""
    print("=== æ¥½å¤© Scraper ãƒ†ã‚¹ãƒˆ ===")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆIDï¼ˆå®Ÿéš›ã®IDã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    test_affiliate_id = "123456789"
    
    try:
        with RakutenScraper(affiliate_id=test_affiliate_id) as scraper:
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            if not scraper.test_connection():
                print("âŒ æ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—")
                return
            
            # å•†å“æ¤œç´¢ãƒ†ã‚¹ãƒˆ
            test_products = ["iPhone", "ãƒã‚¦ã‚¹", "ã‚³ãƒ¼ãƒ’ãƒ¼"]
            
            for product in test_products:
                print(f"\n--- {product} ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ ---")
                results = scraper.search_product(product, max_results=3)
                
                for i, result in enumerate(results, 1):
                    print(f"{i}. {result['title'][:50]}...")
                    print(f"   ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‰: {result['item_code']}")
                    print(f"   URL: {result['affiliate_url'][:80]}...")
                    print()
    
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")


if __name__ == "__main__":
    test_rakuten_scraper()